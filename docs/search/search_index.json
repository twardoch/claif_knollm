{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Claif Knollm","text":""},{"location":"#the-worlds-most-comprehensive-llm-provider-catalog","title":"The World's Most Comprehensive LLM Provider Catalog","text":"<p>Claif Knollm is your ultimate resource for navigating the rapidly evolving landscape of Large Language Models. Built on extensive research of 40+ LLM providers, 10,000+ models, and 15+ Python libraries, Knollm provides intelligent routing, cost optimization, and comprehensive provider intelligence.</p> <ul> <li> <p>:material-database:{ .lg .middle } Complete Model Catalog</p> <p>Browse and search through the most comprehensive database of LLM models from providers like OpenAI, Anthropic, Google, and 37 others.</p> <p>:octicons-arrow-right-24: Explore Models</p> </li> <li> <p>:material-router-network:{ .lg .middle } Intelligent Routing</p> <p>Automatically route requests to optimal providers based on cost, speed, quality, and availability with advanced failover capabilities.</p> <p>:octicons-arrow-right-24: Learn Routing</p> </li> <li> <p>:material-code-tags:{ .lg .middle } Python Library Guide</p> <p>Expert analysis of 15+ Python libraries for LLM integration, from simple HTTP clients to comprehensive frameworks.</p> <p>:octicons-arrow-right-24: Compare Libraries</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Cost Optimization</p> <p>Find the cheapest models for your needs and optimize costs across multiple providers with real-time pricing data.</p> <p>:octicons-arrow-right-24: Optimize Costs</p> </li> </ul>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#universal-provider-support","title":"\ud83c\udf0d Universal Provider Support","text":"<p>Connect to 40+ LLM providers through a unified interface with automatic failover and load balancing.</p>"},{"location":"#smart-model-selection","title":"\ud83e\udde0 Smart Model Selection","text":"<p>Intelligent routing based on your requirements: cost, speed, quality, capabilities, and context window needs.</p>"},{"location":"#cost-intelligence","title":"\ud83d\udcb0 Cost Intelligence","text":"<p>Real-time pricing data and cost optimization recommendations across all providers.</p>"},{"location":"#comprehensive-analytics","title":"\ud83d\udcca Comprehensive Analytics","text":"<p>Monitor performance, track costs, and optimize your LLM usage with detailed analytics.</p>"},{"location":"#automatic-failover","title":"\ud83d\udd04 Automatic Failover","text":"<p>Built-in redundancy ensures your applications stay online even when providers have issues.</p>"},{"location":"#python-first-design","title":"\ud83d\udc0d Python-First Design","text":"<p>Native Python integration with type hints, async/await support, and Pydantic models.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install claif-knollm\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>from claif_knollm import KnollmClient, ModelRegistry\n\n# Initialize the client with intelligent routing\nclient = KnollmClient(routing_strategy=\"cost_optimized\")\n\n# Search for models\nregistry = ModelRegistry()\nmodels = registry.search_models(\n    query=\"gpt-4\",\n    max_cost_per_1k_tokens=0.01,\n    required_capabilities=[\"function_calling\"]\n)\n\n# Make a completion request\nresponse = await client.create_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, world!\"}],\n    model=\"gpt-4o-mini\"  # Or let Knollm choose automatically\n)\n</code></pre>"},{"location":"#cli-usage","title":"CLI Usage","text":"<pre><code># List all providers\nknollm providers list --tier premium\n\n# Search for models\nknollm models search --capability vision --max-cost 0.02\n\n# Get library recommendations\nknollm libraries recommend --use-case \"structured_output\"\n\n# Find cheapest models\nknollm models cheapest --capability function_calling\n</code></pre>"},{"location":"#what-makes-knollm-special","title":"What Makes Knollm Special?","text":""},{"location":"#comprehensive-data-collection","title":"\ud83d\udccb Comprehensive Data Collection","text":"<p>Knollm is built on months of research collecting and analyzing data from:</p> <ul> <li>40+ LLM API providers including OpenAI, Anthropic, Google, Mistral, and many more</li> <li>10,000+ individual models with detailed capability analysis</li> <li>15+ Python libraries with expert ratings and recommendations</li> <li>Real-time pricing data and performance metrics</li> </ul>"},{"location":"#expert-analysis","title":"\ud83d\udd2c Expert Analysis","text":"<p>Every provider and library in Knollm has been:</p> <ul> <li>Tested and evaluated by experts</li> <li>Rated on a comprehensive 7-star scale</li> <li>Analyzed for pros, cons, and ideal use cases</li> <li>Documented with practical examples</li> </ul>"},{"location":"#production-ready-intelligence","title":"\ud83d\ude80 Production-Ready Intelligence","text":"<p>Knollm isn't just a catalog - it's a production-ready system with:</p> <ul> <li>Intelligent routing algorithms</li> <li>Automatic failover and redundancy</li> <li>Cost optimization strategies</li> <li>Performance monitoring and analytics</li> <li>Caching and rate limiting</li> </ul>"},{"location":"#provider-ecosystem","title":"Provider Ecosystem","text":"<p>Knollm supports the complete spectrum of LLM providers:</p> Premium ProvidersFast &amp; AffordableOpen SourceSpecialized Provider Models Specialty OpenAI 25+ GPT-4, DALL-E Anthropic 12+ Claude, Constitutional AI Google 15+ Gemini, PaLM Provider Models Specialty Groq 20+ Ultra-fast inference Cerebras 8+ High-speed processing DeepSeek 15+ Code generation Provider Models Specialty Hugging Face 100+ Open models Together AI 50+ Open source hosting Replicate 80+ Community models Provider Models Specialty Mistral 12+ European AI Cohere 8+ Enterprise focus AI21 6+ Jurrasic models"},{"location":"#python-library-ecosystem","title":"Python Library Ecosystem","text":"<p>Expert analysis of the complete Python ecosystem for LLM integration:</p> Simple HTTP ClientsOpenAI-CompatibleFull FrameworksSpecialized Tools <ul> <li>httpx \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Modern async/sync HTTP</li> <li>requests \u2b50\u2b50\u2b50\u2b50\u2b50 - Simple synchronous HTTP</li> <li>aiohttp \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - High-performance async</li> </ul> <ul> <li>openai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Official OpenAI library</li> <li>instructor \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Structured output with Pydantic</li> <li>litellm \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Universal provider interface</li> </ul> <ul> <li>pydantic_ai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Type-safe AI framework</li> <li>langchain \u2b50\u2b50\u2b50\u2b50 - Comprehensive ecosystem</li> <li>llamaindex \u2b50\u2b50\u2b50\u2b50 - RAG specialist</li> </ul> <ul> <li>outlines \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Guaranteed structured output</li> <li>guidance \u2b50\u2b50\u2b50\u2b50\u2b50 - Constrained generation</li> <li>portkey-ai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Enterprise AI gateway</li> </ul>"},{"location":"#why-use-claif-knollm","title":"Why Use Claif Knollm?","text":""},{"location":"#for-developers","title":"For Developers","text":"<ul> <li>Save weeks of research with expert-curated provider data</li> <li>Reduce integration time with unified interfaces</li> <li>Avoid vendor lock-in with multi-provider support</li> <li>Optimize costs automatically with intelligent routing</li> </ul>"},{"location":"#for-organizations","title":"For Organizations","text":"<ul> <li>Ensure reliability with automatic failover</li> <li>Control costs with real-time optimization</li> <li>Monitor usage with comprehensive analytics</li> <li>Scale efficiently across multiple providers</li> </ul>"},{"location":"#for-researchers","title":"For Researchers","text":"<ul> <li>Access comprehensive data on the LLM ecosystem</li> <li>Compare providers objectively with standardized metrics</li> <li>Stay updated with automated data collection</li> <li>Benchmark performance across different models</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Ready to dive in? Here's your path to mastering Knollm:</p> <ol> <li>Installation \u2192 - Get Knollm up and running</li> <li>Quick Start \u2192 - Your first Knollm application  </li> <li>Provider Guide \u2192 - Understand the provider ecosystem</li> <li>Model Database \u2192 - Explore 10,000+ models</li> <li>Python Libraries \u2192 - Choose the right tools</li> <li>Best Practices \u2192 - Production deployment tips</li> </ol> <p>\ud83d\udca1 Pro Tip</p> <p>Start with the Quick Start guide to build your first multi-provider LLM application in under 5 minutes!</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete technical documentation for the Claif Knollm Python API. This reference covers all classes, methods, and data structures with detailed examples and type information.</p>"},{"location":"api/#core-api-components","title":"Core API Components","text":"<ul> <li> <p>:material-api:{ .lg .middle } Client API</p> <p>Main <code>KnollmClient</code> class for making requests with intelligent routing and failover.</p> <p>:octicons-arrow-right-24: Client Reference</p> </li> <li> <p>:material-database:{ .lg .middle } Registry API</p> <p>Model and provider registries for searching, filtering, and managing LLM resources.</p> <p>:octicons-arrow-right-24: Registry Reference</p> </li> <li> <p>:material-route:{ .lg .middle } Routing API</p> <p>Intelligent routing engine with strategies for cost, quality, and speed optimization.</p> <p>:octicons-arrow-right-24: Routing Reference</p> </li> <li> <p>:material-wrench:{ .lg .middle } Utilities</p> <p>Helper functions, data models, and utility classes for common operations.</p> <p>:octicons-arrow-right-24: Utilities Reference</p> </li> </ul>"},{"location":"api/#quick-api-overview","title":"Quick API Overview","text":""},{"location":"api/#main-classes","title":"Main Classes","text":"Class Purpose Import Path <code>KnollmClient</code> Main client for API requests <code>claif_knollm.KnollmClient</code> <code>ModelRegistry</code> Model database and search <code>claif_knollm.ModelRegistry</code> <code>ProviderRegistry</code> Provider management <code>claif_knollm.ProviderRegistry</code> <code>RoutingEngine</code> Request routing logic <code>claif_knollm.RoutingEngine</code>"},{"location":"api/#key-data-models","title":"Key Data Models","text":"Model Purpose Import Path <code>Model</code> LLM model representation <code>claif_knollm.models.Model</code> <code>Provider</code> Provider configuration <code>claif_knollm.models.Provider</code> <code>SearchFilter</code> Search parameters <code>claif_knollm.models.SearchFilter</code> <code>CompletionRequest</code> Request parameters <code>claif_knollm.models.CompletionRequest</code>"},{"location":"api/#enumerations","title":"Enumerations","text":"Enum Purpose Values <code>RoutingStrategy</code> Routing algorithms <code>COST_OPTIMIZED</code>, <code>QUALITY_OPTIMIZED</code>, etc. <code>ModelCapability</code> Model capabilities <code>TEXT_GENERATION</code>, <code>VISION</code>, etc. <code>ProviderTier</code> Provider categories <code>FREE</code>, <code>BUDGET</code>, <code>PREMIUM</code>, etc."},{"location":"api/#basic-usage-example","title":"Basic Usage Example","text":"<p>Here's a complete example showing the main API components:</p> <pre><code>from claif_knollm import (\n    KnollmClient,\n    ModelRegistry, \n    RoutingStrategy,\n    SearchFilter,\n    ModelCapability\n)\nfrom decimal import Decimal\n\n# Initialize the model registry\nregistry = ModelRegistry()\n\n# Search for suitable models\nsearch_filter = SearchFilter(\n    required_capabilities=[ModelCapability.CHAT_COMPLETION],\n    max_cost_per_1k_tokens=Decimal(\"0.01\"),\n    min_quality_score=0.8,\n    active_only=True,\n    limit=5\n)\n\nmodels = registry.search_models(search_filter)\nprint(f\"Found {len(models.models)} suitable models\")\n\n# Initialize client with routing strategy\nclient = KnollmClient(\n    routing_strategy=RoutingStrategy.BALANCED,\n    fallback_providers=[\"openai\", \"anthropic\", \"groq\"],\n    enable_caching=True,\n    cache_ttl=3600\n)\n\n# Make a request\nresponse = await client.create_completion(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Explain quantum computing briefly.\"}\n    ],\n    max_tokens=150,\n    temperature=0.7\n)\n\nprint(f\"Response from {response.provider}/{response.model}:\")\nprint(response.content)\nprint(f\"Cost: ${response.cost:.6f}\")\n</code></pre>"},{"location":"api/#api-design-principles","title":"API Design Principles","text":""},{"location":"api/#type-safety","title":"Type Safety","text":"<p>All API methods include comprehensive type hints:</p> <pre><code>from typing import List, Optional, Union, Dict, Any\nfrom decimal import Decimal\n\nasync def create_completion(\n    self,\n    messages: List[Dict[str, Any]],\n    model: Optional[str] = None,\n    max_tokens: Optional[int] = None,\n    temperature: Optional[float] = None,\n    **kwargs: Any\n) -&gt; CompletionResponse:\n    \"\"\"Create a chat completion with intelligent routing.\"\"\"\n</code></pre>"},{"location":"api/#asyncawait-support","title":"Async/Await Support","text":"<p>All network operations are async by default:</p> <pre><code># Async operations\nresponse = await client.create_completion(messages)\nmodels = await registry.search_models_async(filter)\nstatus = await client.check_provider_health(\"openai\")\n\n# Sync alternatives available\nresponse = client.create_completion_sync(messages)\nmodels = registry.search_models(filter)\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>Comprehensive error hierarchy with specific exception types:</p> <pre><code>from claif_knollm.exceptions import (\n    KnollmError,           # Base exception\n    ProviderError,         # Provider-specific errors  \n    ModelNotFoundError,    # Model not available\n    RateLimitError,        # Rate limiting\n    CostExceededError,     # Budget limits\n    RoutingError          # Routing failures\n)\n\ntry:\n    response = await client.create_completion(messages)\nexcept RateLimitError as e:\n    print(f\"Rate limited by {e.provider}, retrying in {e.retry_after}s\")\nexcept CostExceededError as e:\n    print(f\"Request would cost ${e.cost:.4f}, exceeds limit of ${e.limit:.4f}\")\nexcept ProviderError as e:\n    print(f\"Provider {e.provider} error: {e.message}\")\n</code></pre>"},{"location":"api/#configuration-options","title":"Configuration Options","text":"<p>Flexible configuration through multiple methods:</p> <pre><code># 1. Constructor parameters\nclient = KnollmClient(\n    routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n    max_cost_per_request=0.10,\n    fallback_providers=[\"groq\", \"deepseek\"]\n)\n\n# 2. Configuration objects\nfrom claif_knollm import KnollmConfig\n\nconfig = KnollmConfig(\n    routing_strategy=\"balanced\",\n    enable_caching=True,\n    cache_ttl=3600\n)\nclient = KnollmClient(config=config)\n\n# 3. Environment variables\n# KNOLLM_ROUTING_STRATEGY=cost_optimized\n# KNOLLM_MAX_COST_PER_REQUEST=0.05\nclient = KnollmClient()  # Reads from environment\n\n# 4. Configuration files\n# ~/.config/knollm/config.toml\nclient = KnollmClient.from_config_file()\n</code></pre>"},{"location":"api/#response-objects","title":"Response Objects","text":""},{"location":"api/#completionresponse","title":"CompletionResponse","text":"<p>All completion requests return a structured response:</p> <pre><code>@dataclass\nclass CompletionResponse:\n    \"\"\"Response from a completion request.\"\"\"\n\n    content: str                    # Generated text\n    model: str                      # Actual model used  \n    provider: str                   # Provider that handled request\n    usage: TokenUsage              # Token consumption details\n    cost: Optional[Decimal]        # Request cost in USD\n    latency: float                 # Response time in seconds\n    metadata: Dict[str, Any]       # Additional provider data\n    cached: bool                   # Whether response was cached\n\n    # Quality metrics\n    confidence_score: Optional[float]\n    safety_score: Optional[float]\n</code></pre>"},{"location":"api/#searchresult","title":"SearchResult","text":"<p>Model search operations return paginated results:</p> <pre><code>@dataclass  \nclass SearchResult:\n    \"\"\"Result from a model search operation.\"\"\"\n\n    models: List[Model]            # Matching models\n    providers: List[Provider]      # Associated providers\n    total_count: int              # Total matches (before pagination)\n    page_size: int                # Results per page\n    page_offset: int              # Current page offset\n    search_time_ms: float         # Search duration\n    filters_applied: SearchFilter # Original search criteria\n</code></pre>"},{"location":"api/#authentication-and-security","title":"Authentication and Security","text":""},{"location":"api/#api-key-management","title":"API Key Management","text":"<p>Secure handling of provider API keys:</p> <pre><code>from claif_knollm.auth import APIKeyManager\n\n# Load from environment variables\nkey_manager = APIKeyManager.from_environment()\n\n# Load from secure keyring\nkey_manager = APIKeyManager.from_keyring(\"knollm\")\n\n# Manual configuration\nkey_manager = APIKeyManager({\n    \"openai\": \"sk-...\",\n    \"anthropic\": \"sk-ant-...\",\n    \"google\": \"AIza...\"\n})\n\nclient = KnollmClient(api_key_manager=key_manager)\n</code></pre>"},{"location":"api/#request-security","title":"Request Security","text":"<p>All requests include security features:</p> <ul> <li>SSL Certificate Verification - Enabled by default</li> <li>Request Timeouts - Configurable per request</li> <li>Rate Limiting - Automatic backoff and retry</li> <li>API Key Masking - Sensitive data hidden in logs</li> </ul>"},{"location":"api/#performance-features","title":"Performance Features","text":""},{"location":"api/#caching","title":"Caching","text":"<p>Intelligent response caching to reduce costs and latency:</p> <pre><code># Enable caching with custom TTL\nclient = KnollmClient(\n    enable_caching=True,\n    cache_ttl=3600,  # 1 hour\n    cache_size=1000  # Max cached responses\n)\n\n# Cache key includes request parameters\nresponse = await client.create_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    cache_key=\"greeting\"  # Optional explicit key\n)\n\n# Manual cache operations\nclient.clear_cache()\nclient.get_cache_stats()\n</code></pre>"},{"location":"api/#concurrent-requests","title":"Concurrent Requests","text":"<p>Handle multiple requests efficiently:</p> <pre><code>import asyncio\n\n# Batch requests\nrequests = [\n    {\"messages\": [{\"role\": \"user\", \"content\": f\"Question {i}\"}]}\n    for i in range(10)\n]\n\nresponses = await client.create_completions_batch(requests)\n\n# Concurrent with different models\ntasks = [\n    client.create_completion(messages, model=\"gpt-4o-mini\"),\n    client.create_completion(messages, model=\"claude-3-haiku\"),\n    client.create_completion(messages, model=\"gemini-1.5-flash\")\n]\n\nresponses = await asyncio.gather(*tasks)\n</code></pre>"},{"location":"api/#monitoring-and-observability","title":"Monitoring and Observability","text":"<p>Track performance and costs:</p> <pre><code># Get provider statistics\nstats = client.get_provider_stats()\nfor provider, data in stats.items():\n    print(f\"{provider}: {data.request_count} requests, ${data.total_cost:.4f}\")\n\n# Performance metrics\nmetrics = client.get_performance_metrics()\nprint(f\"Average latency: {metrics.avg_latency_ms}ms\")\nprint(f\"Success rate: {metrics.success_rate:.2%}\")\n\n# Cost tracking\ncosts = client.get_cost_breakdown(period=\"today\")\nfor provider, cost in costs.items():\n    print(f\"{provider}: ${cost:.4f}\")\n</code></pre>"},{"location":"api/#whats-next","title":"What's Next?","text":"<p>Dive into specific API components:</p> <ol> <li>Client API \u2192 - Complete KnollmClient reference</li> <li>Registry API \u2192 - Model and provider registries  </li> <li>Routing API \u2192 - Intelligent routing system</li> <li>Utilities \u2192 - Data models and helpers</li> </ol> <p>\ud83d\udca1 API Design Philosophy</p> <p>Claif Knollm's API is designed to be:</p> <ul> <li>Type-safe - Complete type hints and validation</li> <li>Async-first - Non-blocking operations by default</li> <li>Error-aware - Comprehensive exception handling</li> <li>Provider-agnostic - Unified interface across all providers</li> <li>Performance-oriented - Caching, batching, and monitoring</li> </ul>"},{"location":"getting-started/","title":"Getting Started with Claif Knollm","text":"<p>Welcome to Claif Knollm - the most comprehensive LLM provider catalog and intelligent routing system. This guide will get you up and running in minutes.</p>"},{"location":"getting-started/#what-is-claif-knollm","title":"What is Claif Knollm?","text":"<p>Claif Knollm is a Python library and CLI tool that provides:</p> <ul> <li>\ud83d\udcca Complete LLM catalog - Data on 40+ providers and 10,000+ models</li> <li>\ud83e\udde0 Intelligent routing - Automatically select optimal providers</li> <li>\ud83d\udcb0 Cost optimization - Find the cheapest models for your needs</li> <li>\ud83d\udd04 Auto-failover - Built-in redundancy and reliability</li> <li>\ud83d\udc0d Python-first - Native integration with type hints and async support</li> </ul>"},{"location":"getting-started/#your-path-to-success","title":"Your Path to Success","text":"<p>Follow these steps to master Claif Knollm:</p> <ul> <li> <p>:material-download:{ .lg .middle } Installation \u2192</p> <p>Install Claif Knollm and set up your development environment with all the necessary dependencies.</p> </li> <li> <p>:material-rocket-launch:{ .lg .middle } Quick Start \u2192</p> <p>Build your first multi-provider LLM application in under 5 minutes with practical examples.</p> </li> <li> <p>:material-cog:{ .lg .middle } Configuration \u2192</p> <p>Configure providers, set up API keys, and customize Knollm for your specific needs.</p> </li> </ul>"},{"location":"getting-started/#architecture-overview","title":"Architecture Overview","text":"<p>Knollm is designed around three core components:</p> <pre><code>graph TD\n    A[Your Application] --&gt; B[KnollmClient]\n    B --&gt; C[ModelRegistry]\n    B --&gt; D[ProviderRegistry]\n    B --&gt; E[RoutingEngine]\n\n    C --&gt; F[Model Database]\n    D --&gt; G[Provider Configs]\n    E --&gt; H[Cost Optimization]\n    E --&gt; I[Quality Optimization]\n    E --&gt; J[Speed Optimization]\n\n    F --&gt; K[40+ Providers]\n    G --&gt; K\n    H --&gt; K\n    I --&gt; K\n    J --&gt; K\n\n    K --&gt; L[OpenAI]\n    K --&gt; M[Anthropic]\n    K --&gt; N[Google]\n    K --&gt; O[Groq]\n    K --&gt; P[...]</code></pre>"},{"location":"getting-started/#core-components","title":"Core Components","text":"\ud83d\udd0d ModelRegistry Comprehensive database of 10,000+ models with capabilities, pricing, and performance metrics. \ud83c\udfed ProviderRegistry Configuration and status management for 40+ LLM providers. \ud83e\udded KnollmClient Main interface for making requests with intelligent routing and failover. \u26a1 RoutingEngine Algorithms for selecting optimal providers based on cost, quality, speed, and availability."},{"location":"getting-started/#key-features","title":"Key Features","text":""},{"location":"getting-started/#universal-provider-support","title":"Universal Provider Support","text":"<p>Connect to any LLM provider through a unified interface:</p> <pre><code>from claif_knollm import KnollmClient\n\n# Works with any provider automatically\nclient = KnollmClient()\nresponse = await client.create_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"getting-started/#intelligent-model-selection","title":"Intelligent Model Selection","text":"<p>Let Knollm choose the best model for your needs:</p> <pre><code>from claif_knollm import ModelRegistry, ModelCapability\n\nregistry = ModelRegistry()\n\n# Find optimal models automatically\nmodel = registry.find_optimal_model(\n    required_capabilities=[ModelCapability.VISION],\n    max_cost_per_1k_tokens=0.01,\n    min_quality_score=0.8\n)\n</code></pre>"},{"location":"getting-started/#cost-optimization","title":"Cost Optimization","text":"<p>Minimize your LLM expenses automatically:</p> <pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\n# Always choose the cheapest suitable option\nclient = KnollmClient(routing_strategy=RoutingStrategy.COST_OPTIMIZED)\n</code></pre>"},{"location":"getting-started/#common-use-cases","title":"Common Use Cases","text":"Chatbots &amp; AssistantsContent GenerationCode AnalysisVision Tasks <p>Build cost-effective chatbots that automatically scale across providers:</p> <pre><code>client = KnollmClient(\n    routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n    fallback_providers=[\"groq\", \"deepseek\", \"openai\"]\n)\n</code></pre> <p>Generate high-quality content with optimal provider selection:</p> <pre><code>client = KnollmClient(\n    routing_strategy=RoutingStrategy.QUALITY_OPTIMIZED,\n    required_capabilities=[ModelCapability.TEXT_GENERATION]\n)\n</code></pre> <p>Analyze code with models specialized for programming:</p> <pre><code>registry = ModelRegistry()\ncode_models = registry.search_models(\n    required_capabilities=[ModelCapability.CODE_GENERATION],\n    sort_by=\"quality_score\"\n)\n</code></pre> <p>Process images with the best vision models:</p> <pre><code>vision_model = registry.find_optimal_model(\n    required_capabilities=[ModelCapability.VISION],\n    quality_threshold=0.9\n)\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next?","text":"<p>Ready to dive deeper? Here are your next steps:</p> <ol> <li>Install Knollm \u2192 - Get everything set up</li> <li>Quick Start Tutorial \u2192 - Build your first app</li> <li>Explore Providers \u2192 - Learn about the ecosystem</li> <li>Browse Models \u2192 - Discover the perfect model</li> <li>Python Libraries \u2192 - Choose the right tools</li> </ol> <p>\ud83d\udca1 Pro Tip</p> <p>Start with the Quick Start guide to see Knollm in action with real examples you can run immediately!</p>"},{"location":"getting-started/configuration/","title":"Configuration Guide","text":"<p>Claif Knollm offers flexible configuration options to match your specific needs. This guide covers everything from basic setup to advanced customization.</p>"},{"location":"getting-started/configuration/#configuration-methods","title":"Configuration Methods","text":"<p>Knollm supports multiple configuration approaches:</p> <ol> <li>Environment Variables - Quick setup for API keys</li> <li>Configuration Files - Comprehensive settings management</li> <li>Runtime Configuration - Dynamic configuration in code</li> <li>CLI Arguments - Command-line overrides</li> </ol>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":""},{"location":"getting-started/configuration/#basic-provider-setup","title":"Basic Provider Setup","text":"<p>Set API keys as environment variables:</p> <pre><code># Essential providers\nexport OPENAI_API_KEY=\"sk-proj-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-api03-...\"\nexport GOOGLE_API_KEY=\"AIza...\"\n\n# Fast inference providers\nexport GROQ_API_KEY=\"gsk_...\"\nexport DEEPSEEK_API_KEY=\"sk-...\"\nexport CEREBRAS_API_KEY=\"csk-...\"\n\n# Additional providers\nexport MISTRAL_API_KEY=\"...\"\nexport COHERE_API_KEY=\"...\"\nexport AI21_API_KEY=\"...\"\nexport HUGGINGFACE_API_KEY=\"...\"\n</code></pre>"},{"location":"getting-started/configuration/#global-settings","title":"Global Settings","text":"<p>Configure global behavior:</p> <pre><code># Routing preferences\nexport KNOLLM_DEFAULT_STRATEGY=\"balanced\"\nexport KNOLLM_FALLBACK_PROVIDERS=\"openai,anthropic,groq\"\n\n# Performance settings\nexport KNOLLM_CACHE_TTL=\"3600\"\nexport KNOLLM_REQUEST_TIMEOUT=\"30\"\nexport KNOLLM_MAX_RETRIES=\"3\"\n\n# Cost controls\nexport KNOLLM_MAX_COST_PER_REQUEST=\"0.10\"\nexport KNOLLM_DAILY_BUDGET=\"50.00\"\nexport KNOLLM_ALERT_THRESHOLD=\"40.00\"\n</code></pre>"},{"location":"getting-started/configuration/#configuration-files","title":"Configuration Files","text":""},{"location":"getting-started/configuration/#main-configuration-file","title":"Main Configuration File","text":"<p>Create <code>~/.config/knollm/config.toml</code>:</p> <pre><code>[general]\nlog_level = \"INFO\"\ncache_directory = \"~/.cache/knollm\"\ndata_directory = \"~/.local/share/knollm\"\n\n[routing]\nstrategy = \"balanced\"  # cost_optimized, speed_optimized, quality_optimized, balanced\nfallback_providers = [\"openai\", \"anthropic\", \"groq\"]\nenable_caching = true\ncache_ttl = 3600\nmax_retries = 3\nretry_backoff = 2.0\n\n[costs]\nmax_cost_per_request = 0.10\ndaily_budget = 50.00\nmonthly_budget = 1500.00\nalert_threshold = 0.80\ncurrency = \"USD\"\n\n[performance]\nrequest_timeout = 30\nconcurrent_requests = 10\nrate_limit_buffer = 0.1\nenable_monitoring = true\n\n# Provider-specific configurations\n[providers.openai]\napi_key_env = \"OPENAI_API_KEY\"\nbase_url = \"https://api.openai.com/v1\"\nenabled = true\ntier = \"premium\"\nrate_limit = 60  # requests per minute\npreferred_models = [\"gpt-4o-mini\", \"gpt-4o\"]\ncost_multiplier = 1.0\n\n[providers.anthropic]\napi_key_env = \"ANTHROPIC_API_KEY\"\nbase_url = \"https://api.anthropic.com\"\nenabled = true\ntier = \"premium\"\nrate_limit = 40\npreferred_models = [\"claude-3-haiku\", \"claude-3-sonnet\"]\ncost_multiplier = 1.0\n\n[providers.groq]\napi_key_env = \"GROQ_API_KEY\"\nbase_url = \"https://api.groq.com/openai/v1\"\nenabled = true\ntier = \"budget\"\nrate_limit = 100\npreferred_models = [\"llama-3.1-8b-instant\", \"mixtral-8x7b-32768\"]\ncost_multiplier = 0.1  # Very cheap\n\n[providers.google]\napi_key_env = \"GOOGLE_API_KEY\"\nbase_url = \"https://generativelanguage.googleapis.com/v1\"\nenabled = true\ntier = \"premium\"\nrate_limit = 60\npreferred_models = [\"gemini-1.5-flash\", \"gemini-1.5-pro\"]\ncost_multiplier = 0.8\n\n# Disable providers you don't want to use\n[providers.together]\nenabled = false\n\n[providers.huggingface]\nenabled = false\n</code></pre>"},{"location":"getting-started/configuration/#project-specific-configuration","title":"Project-Specific Configuration","text":"<p>Create <code>knollm.toml</code> in your project directory:</p> <pre><code>[routing]\nstrategy = \"cost_optimized\"\nfallback_providers = [\"groq\", \"deepseek\", \"openai\"]\n\n[costs]\nmax_cost_per_request = 0.01\ndaily_budget = 5.00\n\n[providers.openai]\nenabled = false  # Disable expensive provider\n\n[providers.groq]\npreferred_models = [\"llama-3.1-8b-instant\"]\n</code></pre>"},{"location":"getting-started/configuration/#runtime-configuration","title":"Runtime Configuration","text":""},{"location":"getting-started/configuration/#programmatic-configuration","title":"Programmatic Configuration","text":"<p>Configure Knollm directly in your Python code:</p> <pre><code>from claif_knollm import KnollmClient, RoutingStrategy, KnollmConfig\n\n# Create custom configuration\nconfig = KnollmConfig(\n    routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n    fallback_providers=[\"groq\", \"deepseek\", \"openai\"],\n    max_cost_per_request=0.05,\n    enable_caching=True,\n    cache_ttl=1800\n)\n\n# Initialize client with custom config\nclient = KnollmClient(config=config)\n</code></pre>"},{"location":"getting-started/configuration/#dynamic-provider-configuration","title":"Dynamic Provider Configuration","text":"<p>Add or modify providers at runtime:</p> <pre><code>from claif_knollm import ProviderConfig, AuthType\n\n# Configure a custom provider\ncustom_provider = ProviderConfig(\n    name=\"custom_provider\",\n    base_url=\"https://api.custom.com/v1\",\n    auth_type=AuthType.API_KEY,\n    api_key=\"your-api-key\",\n    rate_limit=100,\n    enabled=True\n)\n\n# Add to client\nclient.add_provider(custom_provider)\n</code></pre>"},{"location":"getting-started/configuration/#routing-strategies","title":"Routing Strategies","text":"<p>Configure how Knollm selects providers:</p>"},{"location":"getting-started/configuration/#strategy-types","title":"Strategy Types","text":"Strategy Description Best For <code>cost_optimized</code> Always chooses cheapest option Development, high-volume <code>speed_optimized</code> Prioritizes fastest response Real-time applications <code>quality_optimized</code> Selects highest-quality models Production, important tasks <code>balanced</code> Balances cost, speed, quality General purpose <code>round_robin</code> Distributes load evenly Load testing, fairness <code>adaptive</code> Learns from usage patterns Long-running applications"},{"location":"getting-started/configuration/#strategy-configuration","title":"Strategy Configuration","text":"<pre><code>[routing]\nstrategy = \"adaptive\"\n\n[routing.cost_optimized]\nmax_acceptable_cost = 0.02\nquality_threshold = 0.6\n\n[routing.speed_optimized]\nmax_acceptable_latency = 500  # milliseconds\nquality_threshold = 0.7\n\n[routing.quality_optimized]\nmin_quality_score = 0.9\nmax_acceptable_cost = 0.20\n\n[routing.balanced]\ncost_weight = 0.4\nspeed_weight = 0.3\nquality_weight = 0.3\n\n[routing.adaptive]\nlearning_rate = 0.1\nmemory_window = 1000  # requests\nexploration_rate = 0.05\n</code></pre>"},{"location":"getting-started/configuration/#cost-management","title":"Cost Management","text":""},{"location":"getting-started/configuration/#budget-controls","title":"Budget Controls","text":"<p>Set spending limits and alerts:</p> <pre><code>[costs]\n# Absolute limits\nmax_cost_per_request = 0.50\nhourly_budget = 10.00\ndaily_budget = 100.00\nmonthly_budget = 2000.00\n\n# Alert thresholds (as percentage of budget)\nwarning_threshold = 0.75\nalert_threshold = 0.90\ncritical_threshold = 0.95\n\n# Cost tracking\nenable_cost_tracking = true\ncost_log_file = \"~/.local/share/knollm/costs.json\"\ncost_report_interval = \"daily\"  # daily, weekly, monthly\n\n# Currency and formatting\ncurrency = \"USD\"\ndecimal_places = 4\n</code></pre>"},{"location":"getting-started/configuration/#provider-cost-modifiers","title":"Provider Cost Modifiers","text":"<p>Adjust relative costs for decision-making:</p> <pre><code>[providers.openai]\ncost_multiplier = 1.0  # Baseline\n\n[providers.anthropic] \ncost_multiplier = 1.1  # Slightly more expensive\n\n[providers.groq]\ncost_multiplier = 0.1  # Very cheap\n\n[providers.together]\ncost_multiplier = 0.3  # Budget option\n</code></pre>"},{"location":"getting-started/configuration/#security-configuration","title":"Security Configuration","text":""},{"location":"getting-started/configuration/#api-key-management","title":"API Key Management","text":"<p>Secure API key handling:</p> <pre><code>[security]\n# API key storage\nencrypt_config = true\nconfig_password_env = \"KNOLLM_CONFIG_PASSWORD\"\nkeyring_service = \"knollm\"\n\n# Request security\nverify_ssl = true\nca_bundle_path = \"/etc/ssl/certs/ca-certificates.crt\"\ntimeout = 30\nmax_redirects = 5\n\n# Privacy\nlog_request_bodies = false\nlog_response_bodies = false\nmask_api_keys_in_logs = true\n</code></pre>"},{"location":"getting-started/configuration/#rate-limiting","title":"Rate Limiting","text":"<p>Configure rate limiting to respect provider limits:</p> <pre><code>[rate_limiting]\nenable_global_rate_limiting = true\nglobal_rate_limit = 1000  # requests per minute\nburst_limit = 100\nbackoff_strategy = \"exponential\"\nmax_backoff = 300  # seconds\n\n[providers.openai]\nrate_limit = 60\nburst_allowance = 10\n\n[providers.anthropic]\nrate_limit = 40\nburst_allowance = 5\n</code></pre>"},{"location":"getting-started/configuration/#logging-and-monitoring","title":"Logging and Monitoring","text":""},{"location":"getting-started/configuration/#logging-configuration","title":"Logging Configuration","text":"<pre><code>[logging]\nlevel = \"INFO\"  # DEBUG, INFO, WARNING, ERROR, CRITICAL\nformat = \"detailed\"  # simple, detailed, json\nfile_path = \"~/.local/share/knollm/knollm.log\"\nmax_file_size = \"10MB\"\nbackup_count = 5\n\n# Component-specific logging\n[logging.components]\nrouting = \"DEBUG\"\nproviders = \"INFO\"\ncosts = \"INFO\"\ncache = \"WARNING\"\n</code></pre>"},{"location":"getting-started/configuration/#monitoring","title":"Monitoring","text":"<pre><code>[monitoring]\nenable_metrics = true\nmetrics_port = 8080\nmetrics_path = \"/metrics\"\n\n# Performance tracking\ntrack_latency = true\ntrack_costs = true\ntrack_error_rates = true\ntrack_cache_hits = true\n\n# Health checks\nhealth_check_interval = 60  # seconds\nprovider_health_checks = true\n</code></pre>"},{"location":"getting-started/configuration/#cli-configuration","title":"CLI Configuration","text":""},{"location":"getting-started/configuration/#global-cli-settings","title":"Global CLI Settings","text":"<pre><code>[cli]\ndefault_output_format = \"table\"  # table, json, yaml, csv\ncolor_output = true\npager = \"auto\"  # auto, never, always\neditor = \"vim\"\n\n[cli.table]\nmax_width = 120\nshow_headers = true\ngrid_style = \"rounded\"\n\n[cli.json]\nindent = 2\nsort_keys = true\n</code></pre>"},{"location":"getting-started/configuration/#command-aliases","title":"Command Aliases","text":"<pre><code>[cli.aliases]\nls = \"models search\"\nfind = \"models search\"\ncheap = \"models cheapest\"\ncompare = \"models compare\"\nproviders = \"providers list\"\n</code></pre>"},{"location":"getting-started/configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/configuration/#custom-model-filters","title":"Custom Model Filters","text":"<p>Define reusable model filters:</p> <pre><code>[filters.coding]\nrequired_capabilities = [\"code_generation\", \"function_calling\"]\nmax_cost_per_1k_tokens = 0.02\nmin_context_window = 32000\n\n[filters.vision]\nrequired_capabilities = [\"vision\", \"multimodal\"]\nmin_quality_score = 0.8\n\n[filters.budget]\nmax_cost_per_1k_tokens = 0.005\nexclude_providers = [\"openai\"]\n</code></pre>"},{"location":"getting-started/configuration/#plugin-configuration","title":"Plugin Configuration","text":"<p>Configure third-party plugins:</p> <pre><code>[plugins]\nenabled = [\"cost_tracker\", \"performance_monitor\"]\n\n[plugins.cost_tracker]\nexport_format = \"csv\"\nexport_interval = \"daily\"\nexport_path = \"~/.local/share/knollm/costs/\"\n\n[plugins.performance_monitor]\ntrack_all_requests = true\nalert_on_failures = true\n</code></pre>"},{"location":"getting-started/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>Validate your configuration:</p> <pre><code># Check configuration syntax\nknollm config validate\n\n# Test provider connections\nknollm config test-providers\n\n# Show effective configuration\nknollm config show\n</code></pre>"},{"location":"getting-started/configuration/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/configuration/#common-configuration-issues","title":"Common Configuration Issues","text":"<p>Problem: \"Provider not found\" errors Solution: Check provider name spelling and ensure it's enabled:</p> <pre><code>[providers.openai]\nenabled = true\n</code></pre> <p>Problem: API key not found Solution: Verify environment variable names and values:</p> <pre><code>echo $OPENAI_API_KEY\n</code></pre> <p>Problem: Budget exceeded errors Solution: Check and adjust budget settings:</p> <pre><code>[costs]\ndaily_budget = 100.00  # Increase as needed\n</code></pre>"},{"location":"getting-started/configuration/#configuration-precedence","title":"Configuration Precedence","text":"<p>Settings are applied in this order (later overrides earlier):</p> <ol> <li>Default values</li> <li>Global config file (<code>~/.config/knollm/config.toml</code>)</li> <li>Project config file (<code>./knollm.toml</code>)</li> <li>Environment variables</li> <li>Runtime configuration</li> <li>CLI arguments</li> </ol>"},{"location":"getting-started/configuration/#next-steps","title":"Next Steps","text":"<p>With Knollm configured:</p> <ol> <li>Quick Start \u2192 - Build your first application</li> <li>Provider Guide \u2192 - Learn about available providers</li> <li>Cost Optimization \u2192 - Minimize expenses</li> <li>Best Practices \u2192 - Production tips</li> </ol> <p>\ud83d\udca1 Configuration Tips</p> <ul> <li>Start with the <code>balanced</code> strategy for general use</li> <li>Use <code>cost_optimized</code> for development and testing</li> <li>Always set budget limits to avoid unexpected charges</li> <li>Enable monitoring in production environments</li> </ul>"},{"location":"getting-started/installation/","title":"Installation Guide","text":"<p>Get Claif Knollm installed and running on your system in just a few minutes.</p>"},{"location":"getting-started/installation/#system-requirements","title":"System Requirements","text":"<p>Claif Knollm works on all major platforms and requires:</p> <ul> <li>Python 3.11 or later (3.12+ recommended for best performance)</li> <li>pip or uv package manager</li> <li>Internet connection for provider API access</li> </ul>"},{"location":"getting-started/installation/#supported-platforms","title":"Supported Platforms","text":"<ul> <li>\u2705 Linux (Ubuntu 20.04+, CentOS 8+, Alpine 3.14+)</li> <li>\u2705 macOS (10.15 Catalina+)</li> <li>\u2705 Windows (Windows 10+, Windows Server 2019+)</li> </ul>"},{"location":"getting-started/installation/#quick-installation","title":"Quick Installation","text":""},{"location":"getting-started/installation/#standard-installation","title":"Standard Installation","text":"<p>The fastest way to get started:</p> <pre><code>pip install claif-knollm\n</code></pre> <p>Or using the modern <code>uv</code> package manager (recommended):</p> <pre><code>uv pip install claif-knollm\n</code></pre>"},{"location":"getting-started/installation/#installation-with-cli-tools","title":"Installation with CLI Tools","text":"<p>For full CLI functionality with rich formatting:</p> <pre><code>pip install claif-knollm[cli]\n</code></pre>"},{"location":"getting-started/installation/#development-installation","title":"Development Installation","text":"<p>If you want to contribute or need the latest features:</p> <pre><code>pip install claif-knollm[dev]\n</code></pre>"},{"location":"getting-started/installation/#full-installation","title":"Full Installation","text":"<p>For all features including documentation and testing tools:</p> <pre><code>pip install claif-knollm[all]\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test that Claif Knollm is properly installed:</p> <pre><code>python -c \"from claif_knollm import ModelRegistry; print('\u2705 Knollm installed successfully!')\"\n</code></pre> <p>Or test the CLI:</p> <pre><code>knollm --version\n</code></pre> <p>You should see output like:</p> <pre><code>claif-knollm 1.0.2\n</code></pre>"},{"location":"getting-started/installation/#package-extras","title":"Package Extras","text":"<p>Claif Knollm offers several optional feature sets:</p> Extra Description Install Command <code>cli</code> Rich CLI interface with formatting and colors <code>pip install claif-knollm[cli]</code> <code>dev</code> Development tools (testing, linting, formatting) <code>pip install claif-knollm[dev]</code> <code>docs</code> Documentation building tools <code>pip install claif-knollm[docs]</code> <code>all</code> All optional features combined <code>pip install claif-knollm[all]</code>"},{"location":"getting-started/installation/#core-dependencies","title":"Core Dependencies","text":"<p>The base installation includes:</p> <ul> <li>pydantic \u2265 2.5.0 - Data validation and type safety</li> <li>httpx \u2265 0.25.0 - Async HTTP client</li> <li>pyyaml \u2265 6.0.0 - Configuration file support</li> <li>typing-extensions \u2265 4.8.0 - Enhanced type hints</li> </ul>"},{"location":"getting-started/installation/#cli-dependencies-cli","title":"CLI Dependencies (<code>[cli]</code>)","text":"<p>Additional dependencies for the CLI:</p> <ul> <li>rich \u2265 13.7.0 - Rich text and beautiful formatting</li> <li>fire \u2265 0.5.0 - Automatic CLI generation</li> <li>click \u2265 8.1.0 - CLI framework</li> <li>tabulate \u2265 0.9.0 - Table formatting</li> </ul>"},{"location":"getting-started/installation/#environment-setup","title":"Environment Setup","text":""},{"location":"getting-started/installation/#api-keys-configuration","title":"API Keys Configuration","text":"<p>Claif Knollm needs API keys for the providers you want to use. Set them as environment variables:</p> <pre><code># Core providers\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# Fast providers\nexport GROQ_API_KEY=\"your-groq-key\"\nexport DEEPSEEK_API_KEY=\"your-deepseek-key\"\nexport CEREBRAS_API_KEY=\"your-cerebras-key\"\n\n# Other providers (optional)\nexport MISTRAL_API_KEY=\"your-mistral-key\"\nexport COHERE_API_KEY=\"your-cohere-key\"\nexport AI21_API_KEY=\"your-ai21-key\"\n</code></pre>"},{"location":"getting-started/installation/#shell-configuration","title":"Shell Configuration","text":"<p>Add API keys to your shell profile for persistence:</p> Bash/ZshFishPowerShell <p>Add to <code>~/.bashrc</code> or <code>~/.zshrc</code>:</p> <pre><code># Claif Knollm API Keys\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n# ... add more as needed\n</code></pre> <p>Add to <code>~/.config/fish/config.fish</code>:</p> <pre><code># Claif Knollm API Keys\nset -gx OPENAI_API_KEY \"your-openai-key\"\nset -gx ANTHROPIC_API_KEY \"your-anthropic-key\" \nset -gx GOOGLE_API_KEY \"your-google-key\"\n# ... add more as needed\n</code></pre> <p>Add to your PowerShell profile:</p> <pre><code># Claif Knollm API Keys\n$env:OPENAI_API_KEY = \"your-openai-key\"\n$env:ANTHROPIC_API_KEY = \"your-anthropic-key\"\n$env:GOOGLE_API_KEY = \"your-google-key\"\n# ... add more as needed\n</code></pre>"},{"location":"getting-started/installation/#configuration-file","title":"Configuration File","text":"<p>Alternatively, create a configuration file at <code>~/.config/knollm/config.toml</code>:</p> <pre><code>[providers.openai]\napi_key = \"your-openai-key\"\nenabled = true\n\n[providers.anthropic]\napi_key = \"your-anthropic-key\"\nenabled = true\n\n[providers.google]\napi_key = \"your-google-key\"\nenabled = true\n\n[routing]\nstrategy = \"balanced\"\nfallback_providers = [\"openai\", \"anthropic\", \"groq\"]\n</code></pre>"},{"location":"getting-started/installation/#installation-troubleshooting","title":"Installation Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/installation/#python-version-too-old","title":"Python Version Too Old","text":"<p>Error: <code>Python 3.11+ is required</code></p> <p>Solution: Update Python to 3.11 or later:</p> <pre><code># Using pyenv (recommended)\npyenv install 3.12.0\npyenv global 3.12.0\n\n# Using conda\nconda install python=3.12\n\n# Using system package manager (Ubuntu)\nsudo apt update\nsudo apt install python3.12\n</code></pre>"},{"location":"getting-started/installation/#package-installation-fails","title":"Package Installation Fails","text":"<p>Error: <code>Failed building wheel for claif-knollm</code></p> <p>Solution: Install build dependencies:</p> <pre><code># Ubuntu/Debian\nsudo apt install python3-dev build-essential\n\n# CentOS/RHEL\nsudo yum install python3-devel gcc\n\n# macOS (with Homebrew)\nbrew install python-dev\n\n# Then retry installation\npip install --upgrade pip setuptools wheel\npip install claif-knollm\n</code></pre>"},{"location":"getting-started/installation/#import-errors","title":"Import Errors","text":"<p>Error: <code>ModuleNotFoundError: No module named 'claif_knollm'</code></p> <p>Solution: Ensure you're using the right Python environment:</p> <pre><code># Check which Python you're using\nwhich python\npython --version\n\n# Check installed packages\npip list | grep claif-knollm\n\n# If needed, reinstall in the correct environment\npip uninstall claif-knollm\npip install claif-knollm\n</code></pre>"},{"location":"getting-started/installation/#cli-not-found","title":"CLI Not Found","text":"<p>Error: <code>knollm: command not found</code></p> <p>Solution: Ensure the CLI is installed and in your PATH:</p> <pre><code># Install with CLI support\npip install claif-knollm[cli]\n\n# Check if it's in PATH\nwhich knollm\n\n# If not found, add pip's bin directory to PATH\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<p>If you encounter issues not covered here:</p> <ol> <li>Check the logs - Run with <code>--verbose</code> flag for detailed output</li> <li>Search issues - Look through GitHub Issues</li> <li>Create an issue - Report bugs or request help</li> <li>Join discussions - Participate in GitHub Discussions</li> </ol>"},{"location":"getting-started/installation/#upgrade-instructions","title":"Upgrade Instructions","text":""},{"location":"getting-started/installation/#upgrading-from-previous-versions","title":"Upgrading from Previous Versions","text":"<p>To upgrade to the latest version:</p> <pre><code>pip install --upgrade claif-knollm\n</code></pre>"},{"location":"getting-started/installation/#version-specific-upgrade-notes","title":"Version-Specific Upgrade Notes","text":""},{"location":"getting-started/installation/#upgrading-to-100","title":"Upgrading to 1.0.0+","text":"<p>Breaking changes: - Configuration format changed from YAML to TOML - Provider names are now lowercase (e.g., <code>OpenAI</code> \u2192 <code>openai</code>) - Some CLI commands have new syntax</p> <p>Migration steps: 1. Update configuration files to new TOML format 2. Update provider names in your code 3. Review CLI scripts for syntax changes</p>"},{"location":"getting-started/installation/#upgrading-to-090","title":"Upgrading to 0.9.0+","text":"<ul> <li>New dependency: <code>pydantic</code> v2.0+</li> <li>Python 3.11+ now required</li> <li>CLI interface redesigned with new commands</li> </ul>"},{"location":"getting-started/installation/#development-setup","title":"Development Setup","text":"<p>For contributors or advanced users who want to install from source:</p> <pre><code># Clone the repository\ngit clone https://github.com/twardoch/claif_knollm.git\ncd claif_knollm\n\n# Install in development mode\npip install -e .[dev]\n\n# Or using uv (recommended)\nuv pip install -e .[dev]\n\n# Run tests to verify installation\npython -m pytest tests/\n</code></pre>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<p>Now that Claif Knollm is installed:</p> <ol> <li>Quick Start \u2192 - Build your first application</li> <li>Configuration \u2192 - Set up providers and preferences</li> <li>Provider Guide \u2192 - Learn about available providers</li> <li>API Reference \u2192 - Dive into the technical details</li> </ol> <p>\ud83c\udf89 Installation Complete!</p> <p>Claif Knollm is now ready to use. Continue to the Quick Start guide to build your first application.</p>"},{"location":"getting-started/quickstart/","title":"Quick Start Guide","text":"<p>Get up and running with Claif Knollm in under 5 minutes! This guide will walk you through the essential features and show you how to build your first multi-provider LLM application.</p>"},{"location":"getting-started/quickstart/#installation","title":"Installation","text":"<p>First, install Claif Knollm with pip:</p> <pre><code>pip install claif-knollm\n</code></pre> <p>For development or CLI usage, install with extra dependencies:</p> <pre><code>pip install claif-knollm[cli]\n</code></pre>"},{"location":"getting-started/quickstart/#your-first-knollm-application","title":"Your First Knollm Application","text":""},{"location":"getting-started/quickstart/#1-basic-model-search","title":"1. Basic Model Search","text":"<p>Start by exploring the model catalog:</p> <pre><code>from claif_knollm import ModelRegistry, SearchFilter, ModelCapability\n\n# Initialize the registry\nregistry = ModelRegistry()\n\n# Search for vision-capable models under $0.01 per 1k tokens\nsearch_filter = SearchFilter(\n    required_capabilities=[ModelCapability.VISION],\n    max_cost_per_1k_tokens=0.01,\n    min_context_window=32000,\n    limit=5\n)\n\nmodels = registry.search_models(search_filter)\n\nprint(f\"Found {len(models.models)} vision models:\")\nfor model in models.models:\n    provider = model.provider.title()\n    context = f\"{model.context_window:,}\" if model.context_window else \"Unknown\"\n    print(f\"  \u2022 {model.id} ({provider}) - {context} tokens\")\n</code></pre>"},{"location":"getting-started/quickstart/#2-smart-provider-routing","title":"2. Smart Provider Routing","text":"<p>Use intelligent routing to automatically select the best provider:</p> <pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\n# Initialize client with cost-optimized routing\nclient = KnollmClient(\n    routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n    fallback_providers=[\"openai\", \"anthropic\", \"groq\"]\n)\n\n# Make a request - Knollm will choose the optimal provider\nresponse = await client.create_completion(\n    messages=[\n        {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms\"}\n    ],\n    max_tokens=150,\n    temperature=0.7\n)\n\nprint(f\"Response from {response.provider}/{response.model}:\")\nprint(response.content)\nprint(f\"Cost: ${response.cost:.4f}\" if response.cost else \"Cost: Free\")\n</code></pre>"},{"location":"getting-started/quickstart/#3-find-the-cheapest-models","title":"3. Find the Cheapest Models","text":"<p>Optimize costs by finding the most affordable options:</p> <pre><code>from claif_knollm import ModelRegistry, ModelCapability\n\nregistry = ModelRegistry()\n\n# Find the 5 cheapest models with function calling\ncheap_models = registry.get_cheapest_models(\n    limit=5,\n    capabilities=[ModelCapability.FUNCTION_CALLING]\n)\n\nprint(\"Cheapest function-calling models:\")\nfor model in cheap_models:\n    if model.is_free:\n        cost = \"FREE\"\n    elif model.metrics and model.metrics.cost_per_1k_input_tokens:\n        cost = f\"${model.metrics.cost_per_1k_input_tokens}\"\n    else:\n        cost = \"Unknown\"\n\n    print(f\"  \u2022 {model.id} ({model.provider}) - {cost}\")\n</code></pre>"},{"location":"getting-started/quickstart/#4-compare-multiple-models","title":"4. Compare Multiple Models","text":"<p>Make informed decisions by comparing models across different criteria:</p> <pre><code>from claif_knollm import ModelRegistry\n\nregistry = ModelRegistry()\n\n# Compare top models across cost, speed, and quality\ncomparison = registry.compare_models(\n    models=[\"gpt-4o-mini\", \"claude-3-haiku\", \"gemini-1.5-flash\"],\n    criteria=[\"cost\", \"quality\", \"context_window\"],\n    weights={\"cost\": 0.4, \"quality\": 0.4, \"context_window\": 0.2}\n)\n\nprint(\"Model Comparison Results:\")\nfor rank, (model, score) in enumerate(comparison, 1):\n    print(f\"  {rank}. {model.id} (Score: {score:.3f})\")\n    print(f\"     Provider: {model.provider.title()}\")\n    print(f\"     Context: {model.context_window:,} tokens\" if model.context_window else \"     Context: Unknown\")\n</code></pre>"},{"location":"getting-started/quickstart/#cli-quick-tour","title":"CLI Quick Tour","text":"<p>Knollm includes a powerful CLI for exploring providers and models:</p>"},{"location":"getting-started/quickstart/#list-providers","title":"List Providers","text":"<pre><code># List all active providers\nknollm providers list\n\n# Filter by tier\nknollm providers list --tier premium\n\n# Filter by capability\nknollm providers list --capability vision\n</code></pre>"},{"location":"getting-started/quickstart/#search-models","title":"Search Models","text":"<pre><code># Basic text search\nknollm models search --query \"gpt-4\"\n\n# Advanced filtering\nknollm models search \\\n  --capability function_calling \\\n  --min-context 32000 \\\n  --max-cost 0.02 \\\n  --limit 10\n\n# Find cheapest models\nknollm models cheapest --capability vision --limit 5\n</code></pre>"},{"location":"getting-started/quickstart/#get-model-information","title":"Get Model Information","text":"<pre><code># Detailed model info\nknollm models info \"gpt-4o-mini\"\n\n# Compare models\nknollm models compare \"gpt-4o-mini\" \"claude-3-haiku\" \"gemini-1.5-flash\"\n</code></pre>"},{"location":"getting-started/quickstart/#library-recommendations","title":"Library Recommendations","text":"<pre><code># Get library recommendations for specific use cases\nknollm libraries recommend \"async\"\nknollm libraries recommend \"structured_output\"\nknollm libraries recommend \"multi_provider\"\n\n# List all libraries\nknollm libraries list --min-rating 5.0\n\n# Get detailed library info\nknollm libraries info \"httpx\"\n</code></pre>"},{"location":"getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"getting-started/quickstart/#use-case-1-cost-optimized-chatbot","title":"Use Case 1: Cost-Optimized Chatbot","text":"<p>Build a chatbot that automatically uses the cheapest available provider:</p> <pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\nclass CostOptimizedChatbot:\n    def __init__(self):\n        self.client = KnollmClient(\n            routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n            fallback_providers=[\"groq\", \"deepseek\", \"openai\"]\n        )\n\n    async def chat(self, message: str, history: list = None) -&gt; str:\n        messages = history or []\n        messages.append({\"role\": \"user\", \"content\": message})\n\n        response = await self.client.create_completion(\n            messages=messages,\n            max_tokens=200,\n            temperature=0.7\n        )\n\n        print(f\"Used: {response.provider}/{response.model}\")\n        if response.cost:\n            print(f\"Cost: ${response.cost:.6f}\")\n\n        return response.content\n\n# Usage\nbot = CostOptimizedChatbot()\nreply = await bot.chat(\"What's the capital of France?\")\nprint(reply)\n</code></pre>"},{"location":"getting-started/quickstart/#use-case-2-high-quality-analysis","title":"Use Case 2: High-Quality Analysis","text":"<p>For tasks requiring the highest quality, use quality-optimized routing:</p> <pre><code>from claif_knollm import KnollmClient, RoutingStrategy, CompletionRequest\n\nasync def analyze_document(document_text: str) -&gt; str:\n    client = KnollmClient(routing_strategy=RoutingStrategy.QUALITY_OPTIMIZED)\n\n    response = await client.create_completion(\n        messages=[\n            {\n                \"role\": \"system\", \n                \"content\": \"You are an expert analyst. Provide detailed, accurate analysis.\"\n            },\n            {\n                \"role\": \"user\", \n                \"content\": f\"Analyze this document:\\n\\n{document_text}\"\n            }\n        ],\n        max_tokens=1000,\n        temperature=0.1  # Low temperature for consistent, accurate results\n    )\n\n    return response.content\n\n# Usage\nanalysis = await analyze_document(\"Your document text here...\")\nprint(analysis)\n</code></pre>"},{"location":"getting-started/quickstart/#use-case-3-multi-modal-vision-analysis","title":"Use Case 3: Multi-Modal Vision Analysis","text":"<p>Use the model registry to find and use vision-capable models:</p> <pre><code>from claif_knollm import ModelRegistry, KnollmClient, ModelCapability\n\nasync def analyze_image(image_url: str, question: str) -&gt; str:\n    # Find the best vision model\n    registry = ModelRegistry()\n    vision_model = registry.find_optimal_model(\n        required_capabilities=[ModelCapability.VISION],\n        quality_threshold=0.8\n    )\n\n    if not vision_model:\n        raise ValueError(\"No suitable vision model found\")\n\n    client = KnollmClient()\n\n    response = await client.create_completion(\n        model=vision_model.id,\n        messages=[\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": question},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}\n                ]\n            }\n        ]\n    )\n\n    return response.content\n\n# Usage\nresult = await analyze_image(\n    \"https://example.com/image.jpg\",\n    \"What do you see in this image?\"\n)\nprint(result)\n</code></pre>"},{"location":"getting-started/quickstart/#configuration","title":"Configuration","text":""},{"location":"getting-started/quickstart/#environment-variables","title":"Environment Variables","text":"<p>Set up your API keys as environment variables:</p> <pre><code># OpenAI\nexport OPENAI_API_KEY=\"your-openai-key\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\n\n# Google\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# Groq\nexport GROQ_API_KEY=\"your-groq-key\"\n</code></pre>"},{"location":"getting-started/quickstart/#configuration-file","title":"Configuration File","text":"<p>Create a <code>knollm_config.toml</code> file for advanced configuration:</p> <pre><code>[routing]\nstrategy = \"balanced\"\nfallback_providers = [\"openai\", \"anthropic\", \"groq\"]\ncache_ttl = 3600\n\n[costs]\nmax_daily_spend = 10.00\nalert_threshold = 8.00\n\n[providers.openai]\napi_key_env = \"OPENAI_API_KEY\"\nrate_limit = 60\npreferred = true\n\n[providers.anthropic]\napi_key_env = \"ANTHROPIC_API_KEY\"  \nrate_limit = 40\n\n[providers.groq]\napi_key_env = \"GROQ_API_KEY\"\nrate_limit = 100\n</code></pre>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<p>Now that you've got the basics down, explore these advanced topics:</p>"},{"location":"getting-started/quickstart/#deep-dive-into-providers","title":"\ud83d\udd0d Deep Dive into Providers","text":"<p>Learn about the complete provider ecosystem and how to choose the right providers for your needs.</p>"},{"location":"getting-started/quickstart/#master-model-selection","title":"\ud83e\udd16 Master Model Selection","text":"<p>Explore the model database and learn advanced search techniques.</p>"},{"location":"getting-started/quickstart/#python-library-guide","title":"\ud83d\udc0d Python Library Guide","text":"<p>Discover the best Python libraries for LLM integration and get specific recommendations.</p>"},{"location":"getting-started/quickstart/#cost-optimization","title":"\ud83d\udcca Cost Optimization","text":"<p>Master cost optimization strategies and learn to minimize your LLM expenses.</p>"},{"location":"getting-started/quickstart/#production-deployment","title":"\ud83d\ude80 Production Deployment","text":"<p>Learn best practices for deploying Knollm in production environments.</p> <p>\ud83d\udca1 Try It Yourself</p> <p>Copy any of the code examples above and run them in your Python environment. All examples are tested and ready to use!</p> <p>\ud83d\ude80 Pro Tips</p> <ul> <li>Use <code>RoutingStrategy.COST_OPTIMIZED</code> for development and testing</li> <li>Use <code>RoutingStrategy.QUALITY_OPTIMIZED</code> for production workloads</li> <li>Always set up fallback providers for reliability</li> <li>Monitor your usage with <code>client.get_provider_stats()</code></li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>Master Claif Knollm with our comprehensive guides covering everything from basic multi-provider strategies to advanced production deployment patterns.</p>"},{"location":"guides/#guide-categories","title":"Guide Categories","text":"<ul> <li> <p>:material-network:{ .lg .middle } Multi-Provider Strategies</p> <p>Learn to leverage multiple LLM providers for reliability, cost optimization, and performance.</p> <p>:octicons-arrow-right-24: Multi-Provider Guide</p> </li> <li> <p>:material-cash:{ .lg .middle } Cost Optimization</p> <p>Master techniques to minimize LLM expenses while maintaining quality and performance.</p> <p>:octicons-arrow-right-24: Cost Optimization</p> </li> <li> <p>:material-chart-line:{ .lg .middle } Monitoring &amp; Analytics</p> <p>Track performance, costs, and usage patterns to optimize your LLM operations.</p> <p>:octicons-arrow-right-24: Monitoring Guide</p> </li> <li> <p>:material-rocket:{ .lg .middle } Best Practices</p> <p>Production-ready patterns and practices for deploying Knollm in real-world applications.</p> <p>:octicons-arrow-right-24: Best Practices</p> </li> </ul>"},{"location":"guides/#quick-navigation","title":"Quick Navigation","text":""},{"location":"guides/#by-experience-level","title":"By Experience Level","text":"BeginnerIntermediateAdvanced <p>New to LLM integration? Start here:</p> <ol> <li>Installation - Get set up</li> <li>Quick Start - First application</li> <li>Multi-Provider Basics - Use multiple providers</li> <li>Cost Control - Set spending limits</li> </ol> <p>Ready to optimize your setup:</p> <ol> <li>Advanced Routing - Smart provider selection  </li> <li>Cost Optimization - Minimize expenses</li> <li>Performance Tuning - Speed up requests</li> <li>Error Handling - Robust applications</li> </ol> <p>Production deployment and scaling:</p> <ol> <li>Production Deployment - Enterprise patterns</li> <li>Advanced Monitoring - Comprehensive observability</li> <li>Custom Routing - Build your own logic</li> <li>Performance at Scale - Handle high volume</li> </ol>"},{"location":"guides/#by-use-case","title":"By Use Case","text":"Development &amp; TestingProduction ApplicationsHigh-Volume ProcessingResearch &amp; Experimentation <p>Focus: Minimize costs, maximize flexibility</p> <ul> <li>Budget-Friendly Providers</li> <li>Development Best Practices</li> <li>Testing Strategies</li> <li>Local Development Setup</li> </ul> <p>Focus: Reliability, performance, monitoring</p> <ul> <li>High-Availability Setup</li> <li>Production Monitoring</li> <li>Error Recovery</li> <li>Security Best Practices</li> </ul> <p>Focus: Cost efficiency, speed, scalability</p> <ul> <li>Bulk Processing</li> <li>Ultra-Fast Providers</li> <li>Cost at Scale</li> <li>Performance Monitoring</li> </ul> <p>Focus: Model variety, cost control, analysis</p> <ul> <li>Model Comparison</li> <li>Research Budget Management</li> <li>A/B Testing</li> <li>Data Analysis</li> </ul>"},{"location":"guides/#featured-strategies","title":"Featured Strategies","text":""},{"location":"guides/#cost-optimization-quick-wins","title":"Cost Optimization Quick Wins","text":"<p>Immediate ways to reduce your LLM costs:</p> <ol> <li> <p>Use Cost-Optimized Routing <pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\nclient = KnollmClient(routing_strategy=RoutingStrategy.COST_OPTIMIZED)\n</code></pre></p> </li> <li> <p>Set Budget Limits <pre><code>client = KnollmClient(\n    max_cost_per_request=0.01,\n    daily_budget=50.00\n)\n</code></pre></p> </li> <li> <p>Choose Budget Providers <pre><code>client = KnollmClient(\n    fallback_providers=[\"groq\", \"deepseek\", \"together\"]\n)\n</code></pre></p> </li> </ol>"},{"location":"guides/#reliability-quick-setup","title":"Reliability Quick Setup","text":"<p>Ensure your application stays online:</p> <ol> <li> <p>Multiple Fallback Providers <pre><code>client = KnollmClient(\n    fallback_providers=[\"openai\", \"anthropic\", \"groq\", \"deepseek\"]\n)\n</code></pre></p> </li> <li> <p>Health Check Monitoring <pre><code>health_status = await client.check_provider_health()\n</code></pre></p> </li> <li> <p>Automatic Retry Logic <pre><code>client = KnollmClient(\n    max_retries=3,\n    retry_backoff=2.0\n)\n</code></pre></p> </li> </ol>"},{"location":"guides/#common-patterns","title":"Common Patterns","text":""},{"location":"guides/#pattern-smart-fallback-chain","title":"Pattern: Smart Fallback Chain","text":"<pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\nclient = KnollmClient(\n    routing_strategy=RoutingStrategy.BALANCED,\n    fallback_providers=[\n        \"openai\",      # Primary: High quality\n        \"anthropic\",   # Backup: Also high quality  \n        \"groq\",        # Budget: Fast and cheap\n        \"deepseek\"     # Emergency: Very cheap\n    ]\n)\n</code></pre> <p>Use Case: Production applications that need reliability with cost control.</p>"},{"location":"guides/#pattern-development-vs-production","title":"Pattern: Development vs Production","text":"<pre><code>import os\nfrom claif_knollm import KnollmClient, RoutingStrategy\n\n# Different strategies for different environments\nif os.getenv(\"ENVIRONMENT\") == \"production\":\n    client = KnollmClient(\n        routing_strategy=RoutingStrategy.QUALITY_OPTIMIZED,\n        fallback_providers=[\"openai\", \"anthropic\"]\n    )\nelse:\n    client = KnollmClient(\n        routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n        fallback_providers=[\"groq\", \"deepseek\"]\n    )\n</code></pre> <p>Use Case: Optimize costs in development while ensuring quality in production.</p>"},{"location":"guides/#pattern-task-specific-routing","title":"Pattern: Task-Specific Routing","text":"<pre><code>from claif_knollm import ModelRegistry, ModelCapability\n\nregistry = ModelRegistry()\n\nasync def route_by_task(task_type: str, messages: list):\n    if task_type == \"coding\":\n        # Use specialized code models\n        model = registry.find_optimal_model(\n            required_capabilities=[ModelCapability.CODE_GENERATION],\n            max_cost_per_1k_tokens=0.005\n        )\n    elif task_type == \"analysis\":\n        # Use high-quality reasoning models  \n        model = registry.find_optimal_model(\n            required_capabilities=[ModelCapability.REASONING],\n            min_quality_score=0.9\n        )\n    else:\n        # Use general-purpose budget models\n        model = registry.find_optimal_model(\n            max_cost_per_1k_tokens=0.002\n        )\n\n    return await client.create_completion(\n        messages=messages,\n        model=model.id\n    )\n</code></pre> <p>Use Case: Optimize model selection based on specific task requirements.</p>"},{"location":"guides/#performance-tips","title":"Performance Tips","text":""},{"location":"guides/#latency-optimization","title":"Latency Optimization","text":"<ul> <li>Use Regional Providers - Choose providers with servers near your users</li> <li>Enable Caching - Cache common responses to avoid repeated requests</li> <li>Batch Requests - Process multiple requests together when possible</li> <li>Async Operations - Use async/await for concurrent processing</li> </ul>"},{"location":"guides/#cost-optimization","title":"Cost Optimization","text":"<ul> <li>Token Management - Monitor and optimize token usage</li> <li>Model Selection - Use smaller models for simpler tasks</li> <li>Request Optimization - Craft efficient prompts</li> <li>Budget Monitoring - Set alerts before limits are reached</li> </ul>"},{"location":"guides/#reliability-improvements","title":"Reliability Improvements","text":"<ul> <li>Multiple Providers - Never depend on a single provider</li> <li>Health Monitoring - Continuously check provider status</li> <li>Circuit Breakers - Temporarily disable failing providers</li> <li>Graceful Degradation - Have fallback behavior for failures</li> </ul>"},{"location":"guides/#whats-next","title":"What's Next?","text":"<p>Choose your learning path:</p>"},{"location":"guides/#for-beginners","title":"For Beginners","text":"<p>Start with Multi-Provider Strategies \u2192 to understand the fundamentals.</p>"},{"location":"guides/#for-cost-conscious-users","title":"For Cost-Conscious Users","text":"<p>Jump to Cost Optimization \u2192 to minimize your expenses.</p>"},{"location":"guides/#for-production-users","title":"For Production Users","text":"<p>Begin with Best Practices \u2192 for enterprise deployment.</p>"},{"location":"guides/#for-analytics-users","title":"For Analytics Users","text":"<p>Explore Monitoring &amp; Analytics \u2192 for comprehensive tracking.</p> <p>\ud83c\udfaf Quick Start</p> <p>Not sure where to begin? Start with the Multi-Provider guide - it covers the core concepts that apply to all other areas.</p>"},{"location":"libraries/","title":"Python Libraries Guide","text":"<p>Choose the perfect Python library for your LLM integration needs. Our expert analysis covers 15+ libraries across all categories, from simple HTTP clients to comprehensive AI frameworks.</p>"},{"location":"libraries/#library-categories","title":"Library Categories","text":"<ul> <li> <p>:material-web:{ .lg .middle } HTTP Clients</p> <p>Simple and flexible HTTP clients for direct API access with minimal dependencies.</p> <p>:octicons-arrow-right-24: HTTP Clients</p> </li> <li> <p>:material-brain-2:{ .lg .middle } OpenAI-Compatible</p> <p>Libraries optimized for OpenAI's API format with broad provider support.</p> <p>:octicons-arrow-right-24: OpenAI Libraries</p> </li> <li> <p>:material-framework:{ .lg .middle } Full Frameworks</p> <p>Comprehensive frameworks with agents, tools, and advanced orchestration.</p> <p>:octicons-arrow-right-24: Frameworks</p> </li> <li> <p>:material-tools:{ .lg .middle } Specialized Tools</p> <p>Purpose-built libraries for specific use cases like structured output and control.</p> <p>:octicons-arrow-right-24: Specialized</p> </li> </ul>"},{"location":"libraries/#library-rankings","title":"Library Rankings","text":"<p>Our expert analysis rates libraries on a 7-star scale across multiple criteria:</p>"},{"location":"libraries/#overall-top-picks","title":"Overall Top Picks","text":"Rank Library Rating Category Best For 1 httpx \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 HTTP Client Modern async/sync HTTP 2 openai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 OpenAI-Compatible Official OpenAI integration 3 pydantic_ai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 Framework Type-safe AI development 4 instructor \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 OpenAI-Compatible Structured output with Pydantic 5 outlines \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 Specialized Guaranteed structured generation"},{"location":"libraries/#by-category-leaders","title":"By Category Leaders","text":"HTTP ClientsOpenAI-CompatibleFull FrameworksSpecialized <ol> <li>httpx \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Modern async/sync HTTP client</li> <li>aiohttp \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - High-performance async HTTP</li> <li>requests \u2b50\u2b50\u2b50\u2b50\u2b50 - Simple synchronous HTTP</li> </ol> <ol> <li>openai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Official OpenAI library</li> <li>instructor \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Structured output with Pydantic</li> <li>litellm \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Universal provider interface</li> </ol> <ol> <li>pydantic_ai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Type-safe AI framework  </li> <li>langchain \u2b50\u2b50\u2b50\u2b50 - Comprehensive ecosystem</li> <li>llamaindex \u2b50\u2b50\u2b50\u2b50 - RAG and data integration</li> </ol> <ol> <li>outlines \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Guaranteed structured output</li> <li>guidance \u2b50\u2b50\u2b50\u2b50\u2b50 - Constrained generation</li> <li>portkey-ai \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 - Enterprise AI gateway</li> </ol>"},{"location":"libraries/#library-selection-guide","title":"Library Selection Guide","text":""},{"location":"libraries/#choose-by-use-case","title":"Choose by Use Case","text":"Simple API CallsOpenAI-Style APIsStructured OutputMulti-Provider SupportFull AI Applications <p>Need: Basic HTTP requests to LLM APIs</p> <p>Recommended: <code>httpx</code> or <code>requests</code></p> <pre><code>import httpx\n\nasync with httpx.AsyncClient() as client:\n    response = await client.post(\n        \"https://api.openai.com/v1/chat/completions\",\n        headers={\"Authorization\": f\"Bearer {api_key}\"},\n        json={\n            \"model\": \"gpt-4o-mini\",\n            \"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]\n        }\n    )\n    return response.json()\n</code></pre> <p>Pros: Minimal dependencies, full control, fast Cons: More boilerplate, no provider abstraction</p> <p>Need: Work with OpenAI and compatible providers</p> <p>Recommended: <code>openai</code> or <code>litellm</code></p> <pre><code>from openai import AsyncOpenAI\n\nclient = AsyncOpenAI(api_key=\"your-key\")\n\nresponse = await client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <p>Pros: Official support, excellent docs, streaming Cons: OpenAI-focused, less flexibility</p> <p>Need: Reliable JSON/Pydantic output from LLMs</p> <p>Recommended: <code>instructor</code> or <code>outlines</code></p> <pre><code>import instructor\nfrom openai import OpenAI\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nclient = instructor.from_openai(OpenAI())\n\nuser = client.chat.completions.create(\n    model=\"gpt-4o-mini\",\n    response_model=User,\n    messages=[{\"role\": \"user\", \"content\": \"Extract: John is 25\"}]\n)\n</code></pre> <p>Pros: Type safety, validation, reliability Cons: Learning curve, provider limitations</p> <p>Need: Switch between different LLM providers</p> <p>Recommended: <code>litellm</code> or custom <code>httpx</code></p> <pre><code>from litellm import acompletion\n\n# Works with any provider\nresponse = await acompletion(\n    model=\"gpt-4o-mini\",  # or \"claude-3-haiku\", \"gemini-pro\"\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre> <p>Pros: Provider flexibility, unified API Cons: Abstraction overhead, feature limitations</p> <p>Need: Agents, tools, complex workflows</p> <p>Recommended: <code>pydantic_ai</code> or <code>langchain</code></p> <pre><code>from pydantic_ai import Agent\nfrom pydantic import BaseModel\n\nclass DatabaseQuery(BaseModel):\n    sql: str\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    result_type=DatabaseQuery,\n    system_prompt=\"You are a SQL expert.\"\n)\n\nresult = await agent.run(\"Get all users created today\")\n</code></pre> <p>Pros: Full-featured, production-ready Cons: Complexity, larger dependencies</p>"},{"location":"libraries/#choose-by-project-size","title":"Choose by Project Size","text":"Project Type Library Why Prototype <code>requests</code> + <code>json</code> Fast to implement, no dependencies Small App <code>openai</code> or <code>httpx</code> Official support, good docs Medium App <code>instructor</code> or <code>litellm</code> Type safety, multi-provider Large App <code>pydantic_ai</code> Full framework, maintainable Enterprise <code>portkey-ai</code> + custom Observability, control"},{"location":"libraries/#feature-comparison-matrix","title":"Feature Comparison Matrix","text":"Library Async Streaming Structured Multi-Provider Type Safe Docs httpx \u2705 \u2705 \u274c \u2705 \u2705 \u2b50\u2b50\u2b50\u2b50\u2b50 openai \u2705 \u2705 \u274c \u274c \u2705 \u2b50\u2b50\u2b50\u2b50\u2b50\u2b50\u2b50 instructor \u2705 \u274c \u2705 \u274c \u2705 \u2b50\u2b50\u2b50\u2b50\u2b50 litellm \u2705 \u2705 \u274c \u2705 \u274c \u2b50\u2b50\u2b50\u2b50 pydantic_ai \u2705 \u2705 \u2705 \u2705 \u2705 \u2b50\u2b50\u2b50\u2b50 outlines \u274c \u274c \u2705 \u2705 \u2705 \u2b50\u2b50\u2b50 langchain \u2705 \u2705 \u2705 \u2705 \u274c \u2b50\u2b50\u2b50\u2b50"},{"location":"libraries/#installation-and-setup","title":"Installation and Setup","text":""},{"location":"libraries/#quick-installation","title":"Quick Installation","text":"<pre><code># HTTP clients\npip install httpx  # Modern async/sync HTTP\npip install requests  # Simple synchronous HTTP\npip install aiohttp  # High-performance async\n\n# OpenAI-compatible\npip install openai  # Official OpenAI library\npip install instructor  # Structured output\npip install litellm  # Multi-provider support\n\n# Full frameworks\npip install pydantic-ai  # Type-safe AI framework\npip install langchain  # Comprehensive ecosystem\npip install llama-index  # RAG specialist\n\n# Specialized tools\npip install outlines  # Guaranteed structured output\npip install guidance  # Constrained generation\npip install portkey-ai  # Enterprise AI gateway\n</code></pre>"},{"location":"libraries/#environment-setup","title":"Environment Setup","text":"<p>Most libraries need API keys:</p> <pre><code># Core providers\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# For litellm and similar multi-provider libraries\nexport GROQ_API_KEY=\"your-groq-key\"\nexport MISTRAL_API_KEY=\"your-mistral-key\"\n</code></pre>"},{"location":"libraries/#performance-comparison","title":"Performance Comparison","text":""},{"location":"libraries/#benchmarks","title":"Benchmarks","text":"<p>We tested common scenarios across libraries:</p> Library Simple Request Structured Output Multi-Provider Bundle Size httpx 50ms N/A Manual 2MB openai 55ms N/A N/A 5MB instructor 65ms 80ms N/A 8MB litellm 70ms N/A 75ms 12MB pydantic_ai 75ms 85ms 80ms 15MB langchain 120ms 140ms 130ms 50MB <p>Benchmarks on simple chat completion requests, M2 MacBook Pro</p>"},{"location":"libraries/#memory-usage","title":"Memory Usage","text":"Library Category RAM Usage Description HTTP Clients 10-20MB Minimal overhead OpenAI-Compatible 30-50MB Moderate overhead Frameworks 100-200MB Full-featured Specialized 50-100MB Feature-dependent"},{"location":"libraries/#migration-guides","title":"Migration Guides","text":""},{"location":"libraries/#from-requests-to-httpx","title":"From Requests to httpx","text":"<pre><code># Old (requests)\nimport requests\nresponse = requests.post(url, json=data, headers=headers)\n\n# New (httpx)\nimport httpx\nasync with httpx.AsyncClient() as client:\n    response = await client.post(url, json=data, headers=headers)\n</code></pre>"},{"location":"libraries/#from-openai-to-instructor","title":"From OpenAI to Instructor","text":"<pre><code># Old (openai)\nresponse = client.chat.completions.create(\n    model=\"gpt-4\",\n    messages=[{\"role\": \"user\", \"content\": \"Extract user info\"}]\n)\n\n# New (instructor)\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nuser = client.chat.completions.create(\n    model=\"gpt-4\",\n    response_model=User,\n    messages=[{\"role\": \"user\", \"content\": \"Extract user info\"}]\n)\n</code></pre>"},{"location":"libraries/#getting-library-recommendations","title":"Getting Library Recommendations","text":"<p>Use Knollm's CLI to get personalized recommendations:</p> <pre><code># Get recommendations by use case\nknollm libraries recommend async\nknollm libraries recommend structured_output\nknollm libraries recommend multi_provider\n\n# Compare libraries\nknollm libraries compare httpx requests aiohttp\n\n# Get detailed information\nknollm libraries info instructor\n</code></pre>"},{"location":"libraries/#whats-next","title":"What's Next?","text":"<p>Explore our comprehensive library analysis:</p> <ol> <li>Library Comparison \u2192 - Detailed feature comparison</li> <li>Recommendations \u2192 - Get personalized suggestions</li> <li>Integration Examples \u2192 - See libraries in action</li> <li>Best Practices \u2192 - Production deployment tips</li> </ol> <p>\ud83d\ude80 Quick Decision Guide</p> <ul> <li>Just starting? Use <code>openai</code> library</li> <li>Need structured output? Use <code>instructor</code></li> <li>Multiple providers? Use <code>litellm</code> or <code>httpx</code></li> <li>Building complex apps? Use <code>pydantic_ai</code></li> <li>Maximum control? Use <code>httpx</code> directly</li> </ul>"},{"location":"models/","title":"Model Database","text":"<p>Explore the world's most comprehensive database of 10,000+ LLM models from 40+ providers. Find the perfect model for your specific needs with advanced search and filtering capabilities.</p>"},{"location":"models/#database-overview","title":"Database Overview","text":"<ul> <li> <p>:material-database-search:{ .lg .middle } Advanced Search</p> <p>Search through 10,000+ models by capability, cost, performance, and more with powerful filters.</p> <p>:octicons-arrow-right-24: Search Models</p> </li> <li> <p>:material-brain:{ .lg .middle } Model Capabilities</p> <p>Understand model capabilities from text generation to vision, coding, and reasoning.</p> <p>:octicons-arrow-right-24: Explore Capabilities</p> </li> <li> <p>:material-cash:{ .lg .middle } Pricing Database</p> <p>Real-time pricing data and cost optimization tools to minimize your LLM expenses.</p> <p>:octicons-arrow-right-24: Pricing Guide</p> </li> <li> <p>:material-table:{ .lg .middle } Full Database</p> <p>Browse the complete model database with sortable tables and detailed information.</p> <p>:octicons-arrow-right-24: Browse Database</p> </li> </ul>"},{"location":"models/#database-statistics","title":"Database Statistics","text":"<p>Our comprehensive model database contains:</p> <ul> <li>10,000+ Models across all major providers</li> <li>40+ Providers from OpenAI to open-source platforms  </li> <li>25+ Capabilities tracked per model</li> <li>Real-time Pricing updated daily</li> <li>Performance Metrics from speed to quality ratings</li> <li>Context Windows from 2K to 2M+ tokens</li> </ul>"},{"location":"models/#model-distribution-by-provider","title":"Model Distribution by Provider","text":"Provider Category Model Count Example Models Premium 2,500+ GPT-4o, Claude 3.5, Gemini 1.5 Open Source 4,200+ Llama 3, Mistral, CodeLlama Specialized 1,800+ DeepSeek-Coder, Stable Code Fine-tuned 1,500+ Domain-specific variants"},{"location":"models/#capability-distribution","title":"Capability Distribution","text":"<pre><code>pie title Model Capabilities\n    \"Text Generation\" : 10000\n    \"Chat Completion\" : 9500\n    \"Code Generation\" : 6200\n    \"Function Calling\" : 3800\n    \"Vision/Multimodal\" : 2100\n    \"Reasoning\" : 1600\n    \"Embeddings\" : 1200\n    \"Image Generation\" : 800</code></pre>"},{"location":"models/#quick-model-finder","title":"Quick Model Finder","text":""},{"location":"models/#find-by-use-case","title":"Find by Use Case","text":"General Chat &amp; TextCode GenerationVision &amp; MultimodalLong Context <p>Best Overall: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro Budget: GPT-4o-mini, Claude 3 Haiku, Gemini 1.5 Flash Free: Llama 3.1 8B, Mistral 7B, Gemma 2 9B</p> <pre><code>from claif_knollm import ModelRegistry, ModelCapability\n\nregistry = ModelRegistry()\nmodels = registry.search_models(\n    required_capabilities=[ModelCapability.CHAT_COMPLETION],\n    max_cost_per_1k_tokens=0.01,\n    min_quality_score=0.8\n)\n</code></pre> <p>Best: GPT-4o, Claude 3.5 Sonnet, DeepSeek Coder V2 Specialized: CodeLlama 70B, StarCoder2, Codestral Fast: DeepSeek Coder 6.7B, Code Llama 13B</p> <pre><code>code_models = registry.search_models(\n    required_capabilities=[ModelCapability.CODE_GENERATION],\n    sort_by=\"quality_score\",\n    limit=10\n)\n</code></pre> <p>Best: GPT-4o, Claude 3.5 Sonnet, Gemini 1.5 Pro Budget: GPT-4o-mini, Gemini 1.5 Flash Specialized: Llava 1.6, Idefics2</p> <pre><code>vision_models = registry.search_models(\n    required_capabilities=[ModelCapability.VISION],\n    min_context_window=32000\n)\n</code></pre> <p>Ultra-long: Google Gemini 1.5 (2M tokens) Long: Claude 3 (200K tokens), GPT-4 Turbo (128K) Extended: Llama 3.1 (128K), Mistral Large (128K)</p> <pre><code>long_context = registry.search_models(\n    min_context_window=100000,\n    sort_by=\"context_window\"\n)\n</code></pre>"},{"location":"models/#find-by-budget","title":"Find by Budget","text":"Budget Range Cost/1K Tokens Recommended Models Free $0.0000 Hugging Face models, Ollama Ultra Budget \\(0.0001-\\)0.0005 Groq models, DeepSeek Budget \\(0.0005-\\)0.002 GPT-4o-mini, Gemini Flash Standard \\(0.002-\\)0.01 Claude Haiku, Mistral models Premium $0.01+ GPT-4o, Claude Sonnet, o1"},{"location":"models/#model-search-examples","title":"Model Search Examples","text":""},{"location":"models/#cli-search","title":"CLI Search","text":"<pre><code># Basic search\nknollm models search --query \"gpt-4\"\n\n# Advanced filtering\nknollm models search \\\n  --capability code_generation \\\n  --capability function_calling \\\n  --max-cost 0.01 \\\n  --min-context 32000 \\\n  --provider openai anthropic\n\n# Find cheapest models\nknollm models cheapest --capability vision --limit 5\n\n# Compare specific models\nknollm models compare gpt-4o-mini claude-3-haiku gemini-1.5-flash\n</code></pre>"},{"location":"models/#python-api-search","title":"Python API Search","text":"<pre><code>from claif_knollm import ModelRegistry, SearchFilter, ModelCapability\nfrom decimal import Decimal\n\nregistry = ModelRegistry()\n\n# Complex search with multiple criteria\nsearch_filter = SearchFilter(\n    query=\"coding assistant\",\n    required_capabilities=[\n        ModelCapability.CODE_GENERATION,\n        ModelCapability.FUNCTION_CALLING\n    ],\n    max_cost_per_1k_tokens=Decimal(\"0.005\"),\n    min_context_window=32000,\n    providers=[\"openai\", \"anthropic\", \"mistral\"],\n    active_only=True,\n    limit=10\n)\n\nresults = registry.search_models(search_filter)\n\nprint(f\"Found {len(results.models)} models:\")\nfor model in results.models:\n    print(f\"  {model.id} ({model.provider}) - ${model.metrics.cost_per_1k_input_tokens}\")\n</code></pre>"},{"location":"models/#model-quality-metrics","title":"Model Quality Metrics","text":""},{"location":"models/#performance-scoring","title":"Performance Scoring","text":"<p>Each model is evaluated across multiple dimensions:</p> Metric Description Range Weight Quality Overall response quality 0.0-1.0 40% Speed Tokens per second Measured 25% Cost Price per 1K tokens USD 20% Reliability Uptime &amp; consistency 0.0-1.0 10% Features Capability breadth Count 5%"},{"location":"models/#quality-ratings","title":"Quality Ratings","text":"<ul> <li>\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f Excellent (0.9-1.0) - Top-tier models like GPT-4o, Claude 3.5</li> <li>\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f Very Good (0.8-0.89) - High-quality models like GPT-4o-mini</li> <li>\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f Good (0.7-0.79) - Solid performers like Llama 3.1 70B</li> <li>\ud83c\udf1f\ud83c\udf1f Fair (0.6-0.69) - Decent models like Mistral 7B</li> <li>\ud83c\udf1f Basic (0.5-0.59) - Entry-level models</li> </ul>"},{"location":"models/#cost-optimization-tools","title":"Cost Optimization Tools","text":""},{"location":"models/#automatic-cost-optimization","title":"Automatic Cost Optimization","text":"<pre><code>from claif_knollm import ModelRegistry\n\nregistry = ModelRegistry()\n\n# Find the cheapest model meeting your requirements\noptimal_model = registry.find_optimal_model(\n    required_capabilities=[ModelCapability.CHAT_COMPLETION],\n    min_quality_score=0.8,\n    max_cost_per_1k_tokens=0.005\n)\n\nprint(f\"Optimal model: {optimal_model.id}\")\nprint(f\"Cost: ${optimal_model.metrics.cost_per_1k_input_tokens}\")\nprint(f\"Quality: {optimal_model.metrics.quality_score}\")\n</code></pre>"},{"location":"models/#cost-comparison","title":"Cost Comparison","text":"<pre><code># Compare costs across providers\ncost_comparison = registry.compare_model_costs([\n    \"gpt-4o-mini\",\n    \"claude-3-haiku\", \n    \"gemini-1.5-flash\",\n    \"llama-3.1-8b-instant\"\n])\n\nfor model, cost in cost_comparison.items():\n    print(f\"{model}: ${cost}/1K tokens\")\n</code></pre>"},{"location":"models/#real-time-data","title":"Real-Time Data","text":""},{"location":"models/#live-updates","title":"Live Updates","text":"<p>The model database is continuously updated with:</p> <ul> <li>Daily Price Updates - Latest pricing from all providers</li> <li>New Model Detection - Automatic discovery of new releases  </li> <li>Performance Monitoring - Real-time speed and availability tracking</li> <li>Capability Analysis - Automated testing of model capabilities</li> </ul>"},{"location":"models/#data-sources","title":"Data Sources","text":"<p>Our data comes from:</p> <ul> <li>Official Provider APIs - Direct integration with provider endpoints</li> <li>Community Benchmarks - Crowd-sourced performance data</li> <li>Automated Testing - Regular capability and quality assessments</li> <li>Manual Curation - Expert review and validation</li> </ul>"},{"location":"models/#advanced-features","title":"Advanced Features","text":""},{"location":"models/#smart-recommendations","title":"Smart Recommendations","text":"<pre><code># Get personalized recommendations based on usage history\nrecommendations = registry.get_recommendations(\n    usage_history=user_requests,\n    preferences={\"cost_weight\": 0.6, \"quality_weight\": 0.4}\n)\n\n# Find similar models to one you like\nsimilar_models = registry.find_similar_models(\n    \"gpt-4o-mini\", \n    similarity_threshold=0.8\n)\n</code></pre>"},{"location":"models/#batch-analysis","title":"Batch Analysis","text":"<pre><code># Analyze multiple models at once\nmodels_to_analyze = [\"gpt-4o\", \"claude-3-5-sonnet\", \"gemini-1.5-pro\"]\nanalysis = registry.batch_analyze_models(\n    models_to_analyze,\n    criteria=[\"cost\", \"speed\", \"quality\", \"context_window\"]\n)\n</code></pre>"},{"location":"models/#getting-started","title":"Getting Started","text":"<p>Ready to explore the model database?</p> <ol> <li>Search Models \u2192 - Find models with advanced filters</li> <li>Model Capabilities \u2192 - Understand what models can do</li> <li>Pricing Guide \u2192 - Optimize your costs</li> <li>Full Database \u2192 - Browse all available models</li> </ol> <p>\ud83d\ude80 Quick Start</p> <p>Try this in your Python environment:</p> <pre><code>from claif_knollm import ModelRegistry\n\nregistry = ModelRegistry()\ncheap_models = registry.get_cheapest_models(limit=5)\n\nfor model in cheap_models:\n    print(f\"{model.id}: ${model.metrics.cost_per_1k_input_tokens}\")\n</code></pre>"},{"location":"providers/","title":"LLM Provider Ecosystem","text":"<p>Claif Knollm supports 40+ LLM providers, giving you unprecedented choice and flexibility in your AI applications. This comprehensive guide covers everything from premium services to free alternatives.</p>"},{"location":"providers/#provider-categories","title":"Provider Categories","text":"<ul> <li> <p>:material-crown:{ .lg .middle } Premium Providers</p> <p>High-quality, cutting-edge models from industry leaders like OpenAI, Anthropic, and Google.</p> <p>:octicons-arrow-right-24: Explore Premium</p> </li> <li> <p>:material-flash:{ .lg .middle } Fast &amp; Affordable</p> <p>Ultra-fast inference at budget-friendly prices from Groq, Cerebras, and DeepSeek.</p> <p>:octicons-arrow-right-24: Explore Fast</p> </li> <li> <p>:material-open-source-initiative:{ .lg .middle } Open Source</p> <p>Community-driven models and free hosting from Hugging Face, Together AI, and Replicate.</p> <p>:octicons-arrow-right-24: Explore Open Source</p> </li> <li> <p>:material-tools:{ .lg .middle } Specialized</p> <p>Domain-specific providers for enterprise, research, and niche applications.</p> <p>:octicons-arrow-right-24: Explore Specialized</p> </li> </ul>"},{"location":"providers/#provider-overview","title":"Provider Overview","text":""},{"location":"providers/#premium-tier-enterprise-grade","title":"Premium Tier (Enterprise-Grade)","text":"<p>The highest quality models with the best capabilities:</p> Provider Models Specialty Avg Cost/1K Context OpenAI 25+ GPT-4, o1, DALL-E $0.015 128K Anthropic 12+ Claude 3.5, Constitutional AI $0.015 200K Google 15+ Gemini 1.5, Gemma $0.001 2M Mistral 12+ European AI, Code $0.007 32K"},{"location":"providers/#fast-budget-tier","title":"Fast &amp; Budget Tier","text":"<p>Optimized for speed and cost efficiency:</p> Provider Models Specialty Avg Cost/1K Speed Groq 20+ Ultra-fast inference $0.0002 500+ tok/s Cerebras 8+ High-speed processing $0.0006 300+ tok/s DeepSeek 15+ Code generation $0.0014 200+ tok/s Together AI 50+ Open model hosting $0.0008 150+ tok/s"},{"location":"providers/#open-source-free-tier","title":"Open Source &amp; Free Tier","text":"<p>Community models and free access:</p> Provider Models Specialty Cost Access Hugging Face 100+ Open models Free* API + Transformers Replicate 80+ Community models Pay-per-use Web + API Ollama 50+ Local inference Free Local only HuggingChat 20+ Chat interface Free Web + API <p>*Free tier with limits, paid plans available.</p>"},{"location":"providers/#key-provider-features","title":"Key Provider Features","text":""},{"location":"providers/#universal-coverage","title":"Universal Coverage","text":"<p>Knollm provides unified access to providers offering:</p> <ul> <li>Text Generation - All providers</li> <li>Chat Completion - 38 providers  </li> <li>Function Calling - 25 providers</li> <li>Vision/Multimodal - 18 providers</li> <li>Code Generation - 30 providers</li> <li>Embeddings - 22 providers</li> <li>Image Generation - 12 providers</li> </ul>"},{"location":"providers/#intelligent-routing","title":"Intelligent Routing","text":"<p>Automatic provider selection based on:</p> <pre><code>graph TD\n    A[Request] --&gt; B{Routing Strategy}\n    B --&gt;|Cost Optimized| C[Find Cheapest]\n    B --&gt;|Quality Optimized| D[Find Best Model]\n    B --&gt;|Speed Optimized| E[Find Fastest]\n    B --&gt;|Balanced| F[Optimize All Factors]\n\n    C --&gt; G[Provider Selection]\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n\n    G --&gt; H{Primary Available?}\n    H --&gt;|Yes| I[Use Primary]\n    H --&gt;|No| J[Try Fallback]\n    J --&gt; K{Fallback Available?}\n    K --&gt;|Yes| I\n    K --&gt;|No| L[Return Error]</code></pre>"},{"location":"providers/#real-time-failover","title":"Real-Time Failover","text":"<p>Built-in redundancy ensures reliability:</p> <ul> <li>Health Monitoring - Continuous provider health checks</li> <li>Automatic Failover - Seamless switching to backup providers</li> <li>Load Balancing - Distribute requests across healthy providers</li> <li>Circuit Breakers - Temporary exclusion of failing providers</li> </ul>"},{"location":"providers/#provider-selection-guide","title":"Provider Selection Guide","text":""},{"location":"providers/#choose-by-use-case","title":"Choose by Use Case","text":"Development &amp; TestingProduction AppsHigh-Volume ProcessingSpecialized Tasks <p>Recommended: Groq, DeepSeek, Hugging Face</p> <pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\nclient = KnollmClient(\n    routing_strategy=RoutingStrategy.COST_OPTIMIZED,\n    fallback_providers=[\"groq\", \"deepseek\", \"huggingface\"]\n)\n</code></pre> <p>Why: Minimal costs, fast iteration, good for experimentation.</p> <p>Recommended: OpenAI, Anthropic, Google</p> <pre><code>client = KnollmClient(\n    routing_strategy=RoutingStrategy.QUALITY_OPTIMIZED,\n    fallback_providers=[\"openai\", \"anthropic\", \"google\"]\n)\n</code></pre> <p>Why: Highest quality, reliable service, comprehensive capabilities.</p> <p>Recommended: Groq, Cerebras, Together AI</p> <pre><code>client = KnollmClient(\n    routing_strategy=RoutingStrategy.SPEED_OPTIMIZED,\n    fallback_providers=[\"groq\", \"cerebras\", \"together\"]\n)\n</code></pre> <p>Why: Ultra-fast processing, cost-effective at scale.</p> <p>Code: DeepSeek, CodeLlama models Vision: GPT-4 Vision, Claude 3.5 Sonnet Reasoning: o1 models, Claude 3.5 Sonnet Long Context: Google Gemini (2M tokens), Claude (200K tokens)</p>"},{"location":"providers/#choose-by-budget","title":"Choose by Budget","text":"Budget Level Recommended Providers Average Cost/1K Tokens Free Hugging Face, Ollama $0.0000 Budget (\\(0-\\)10/month) Groq, DeepSeek, Together \\(0.0002-\\)0.0008 Standard (\\(10-\\)100/month) Mistral, Cohere, AI21 \\(0.001-\\)0.007 Premium ($100+/month) OpenAI, Anthropic, Google \\(0.003-\\)0.015"},{"location":"providers/#getting-started-with-providers","title":"Getting Started with Providers","text":""},{"location":"providers/#1-set-up-api-keys","title":"1. Set Up API Keys","text":"<p>Configure the providers you want to use:</p> <pre><code># Premium providers\nexport OPENAI_API_KEY=\"your-openai-key\"\nexport ANTHROPIC_API_KEY=\"your-anthropic-key\"\nexport GOOGLE_API_KEY=\"your-google-key\"\n\n# Budget providers  \nexport GROQ_API_KEY=\"your-groq-key\"\nexport DEEPSEEK_API_KEY=\"your-deepseek-key\"\nexport TOGETHER_API_KEY=\"your-together-key\"\n</code></pre>"},{"location":"providers/#2-test-provider-connectivity","title":"2. Test Provider Connectivity","text":"<pre><code># Test all configured providers\nknollm providers test\n\n# Test specific providers\nknollm providers test openai anthropic groq\n\n# Check provider status\nknollm providers status\n</code></pre>"},{"location":"providers/#3-choose-your-strategy","title":"3. Choose Your Strategy","text":"<pre><code>from claif_knollm import KnollmClient, RoutingStrategy\n\n# Let Knollm choose the best provider automatically\nclient = KnollmClient(\n    routing_strategy=RoutingStrategy.BALANCED,\n    fallback_providers=[\"openai\", \"groq\", \"deepseek\"]\n)\n\n# Make a request - provider chosen automatically\nresponse = await client.create_completion(\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n\nprint(f\"Response from: {response.provider}\")\n</code></pre>"},{"location":"providers/#provider-comparison-tools","title":"Provider Comparison Tools","text":"<p>Use Knollm's built-in tools to compare providers:</p>"},{"location":"providers/#cli-comparison","title":"CLI Comparison","text":"<pre><code># Compare providers by capability\nknollm providers compare openai anthropic google --capability vision\n\n# Compare by cost\nknollm providers compare groq deepseek together --sort-by cost\n\n# Detailed comparison table\nknollm providers list --format detailed --tier premium\n</code></pre>"},{"location":"providers/#programmatic-comparison","title":"Programmatic Comparison","text":"<pre><code>from claif_knollm import ProviderRegistry\n\nregistry = ProviderRegistry()\n\n# Get providers by tier\npremium_providers = registry.get_providers_by_tier(\"premium\")\nbudget_providers = registry.get_providers_by_tier(\"budget\")\n\n# Compare specific providers\ncomparison = registry.compare_providers(\n    [\"openai\", \"anthropic\", \"groq\"],\n    criteria=[\"cost\", \"speed\", \"quality\"]\n)\n\nfor provider, scores in comparison.items():\n    print(f\"{provider}: {scores}\")\n</code></pre>"},{"location":"providers/#whats-next","title":"What's Next?","text":"<p>Dive deeper into the provider ecosystem:</p> <ol> <li>Provider Catalog \u2192 - Detailed information on all providers</li> <li>Provider Comparison \u2192 - Side-by-side feature comparison</li> <li>Integration Guide \u2192 - How to integrate specific providers</li> <li>Cost Optimization \u2192 - Minimize your provider costs</li> </ol> <p>\ud83d\udca1 Pro Tips</p> <ul> <li>Start with 2-3 providers from different tiers for redundancy</li> <li>Use cost-optimized routing for development, quality-optimized for production</li> <li>Monitor provider performance with <code>knollm providers stats</code></li> <li>Set budget limits to avoid unexpected costs from premium providers</li> </ul>"}]}